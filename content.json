{"pages":[{"title":"","text":"google-site-verification: google880fba41369246da.html","link":"/google880fba41369246da"},{"title":"关于我的博客","text":"⭐✨电赛结束后想着把大学参加竞赛学到的东西记录下来，留作一份纪念，因此想到创建个人博客来记录这些。另外，今后通过写博客可以帮助自己更好地巩固学到的知识📖，还可以提高自己的文字表达能力📝，或许还能对他人有所帮助😄。 以下是我搭建博客过程中参考的内容，十分感谢这些作者 搭建参考1 https://www.jianshu.com/p/ea5ac6162a96 搭建参考2 https://www.bilibili.com/video/av44544186/ 问题(ssh连接不上)解决 https://my.oschina.net/stefanzhlg/blog/529403 问题(空主题)解决 https://www.zhihu.com/question/38781463 主题推荐1 https://github.com/litten/hexo-theme-yilia 主题推荐2 https://volantis.js.org/getting-started/ 主题推荐3 https://github.com/ppoffice/hexo-theme-icarus ICARUS 主题美化 https://www.alphalxy.com/2019/03/customize-icarus/","link":"/about/"}],"posts":[{"title":"Key Metrics and Design Objectives for DNN Accelerator","text":"本文详细介绍了评估DNN加速器时需要考虑的指标和影响因素，并讨论了在设计DNN加速器时应该如何做trade-off。 Key Metrics and Design Objectives目前，深度学习加速器不断涌现，各种“XPU”深度学习加速器都有自己独特的架构优化，那么我们应该如何评估一款加速器的性能以及比较不同DNN加速器之间的好坏呢？或者说在设计一款DNN加速器时需要考虑到哪些因素呢？通常，一款通用处理器的性能我们往往用$FLOPS/W$或者$TOPS/W$来表示，但只关注加速器的性能是不够全面的，我们需要一个全面并且客观的体系来评估加速器各方面的特点，因此研究DNN加速器的评估指标，制定相应的评估系统就显得十分重要。在DNN加速器的设计过程中，加速器设计的好坏与许多因素有关，主要包括 精度（accuracy） 吞吐率（throughput） 延迟（latency） 能效（energy efficiency） 功耗（power） 成本（cost） 灵活性（flexibility） 可拓展性（scalability） 下面我们将详细介绍这些因素是如何影响DNN加速器的性能和能耗，以及讨论我们在设计DNN加速器时应该如何做折中考虑（trade-off）。 Accuracy准确率（accuracy）是对给定任务下识别结果质量的评判。在不用任务下，准确率的定义是不一样的。例如，在图像分类任务中，准确率的定义是正确分类图片的百分比；在目标检测任务中，准确率是各类AP值的平均值（mean average precision，mAP）。 准确率的影响因素主要包括任务难度和数据集。例如，在ImageNet上的分类任务比在MNIST上的分类任务要难很多，因此准确率会低很多；目标检测任务和语意分割任务会比图像分类任务要复杂很多，因此准确率也会低一些。 Throughput吞吐率（Throughput）表示在给定时间下处理器能够处理数据量的大小或执行任务的数量。即表示在单位时间下能够执行计算的数量。通常，吞吐率的表达式为：$$Throughput=\\frac{operations}{second}$$ 在深度学习推理计算过程中，其执行的操作数等效为推理（inference）的次数，因此，吞吐率可以表示为：$$Throughput=\\frac{inferences}{second}\\tag{1}$$ 高吞吐率是我们想要的，因为在一些实时任务下，对数据的实时处理要求较高，高吞吐率意味着在单位时间内我们能够处理的数据更多，那么我们就可以实现高帧率或者高性能处理数据，这在无人驾驶、金融、国防安全等领域有着重要的意义。 那么我们该如何设计高吞吐率的处理器呢？我们将吞吐率的公式进行分解，具体如下：$$\\frac{inferences}{second}={\\frac{operations}{second}}\\times{\\frac{1}{\\frac{operations}{inference}}}\\tag{2}$$ 其中，operations per second 由DNN加速器和DNN模型决定；operations per inference 由DNN模型决定； attention：式中这两个operations是不一样的。细心的读者可能会发现，如果将两个operations等同消掉，将右边式子还原，分子就会变成inference，而左边分子是inferences，那么两边就不相等了。为什么会这样？这是因为前面operations per second中的operations是包含倍数因子的，和inferences类似。举个例子，inferences这个值可以改变的，一般是大于1，而inference是单次推理（即为1），因此inferences对inference有个倍数关系。同理，operations per inference中的operations是由模型确定的，如果模型确定就是一个常量，但是前面operations per second中的operations肯定不是一个常量，它是受硬件设计影响的，两者存在一个倍数关系，因此综上所述，式中这两个operations是不相等的，不能等同处理。 作为DNN加速器的硬件设计者，我们主要关注 operations per second 这个项，它可以进一步分解为：$$\\frac{operations}{second}=(\\frac{1}{\\frac{cycles}{operation}}\\times{\\frac{cycles}{second}})\\times PE_s\\times PE_s\\ utilization\\tag{3}$$ 其中，第一项（括号）反映的是单个PE的峰值吞吐率，它由 cycles per operation 和 cycles per second 组成；第二项是PE单元的数量，反映计算的并行程度；第三项是PE单元的利用率，它由DNN加速器的架构设计决定。 对于第一项而言，cycles per operation 是由PE单元中MAC的设计决定的，如采用流水线结构的MAC每个 operation 需要的 cycles 就会很低，cycles per second 即处理器主频$f$(MHz)，可以通过优化电路的关键路径来提高。 对于第二项而言，PEs的数量提升，会有更多的PE单元同时参与计算，会提高整个加速器的吞吐率。但是在整个芯片面积给定和单个PE面积不变的情况下，PE单元的数量增加通过增加PE阵列的面积，那么会造成片上存储面积的减少，这会进一步影响PE单元的利用率，从而影响到整个加速器的吞吐率。 对于第三项，我们单独来讨论，因为这涉及到DNN加速器的架构，与我们的设计有很大关系，也是我们关注的重点。理想情况下，当PE的利用率达到100%时，尽可能地提高PE的数量和单个PE的峰值吞吐率，可以实现吞吐率的最大值。但是事实上，PE的利用率并不能完全达到100%,实际的吞吐率也取决于实际PE的利用率。对于PE的利用率，它可以分解为：$$PE_s\\ utilization=\\frac{number\\ of\\ active PE_s}{number\\ of\\ PE_s}\\times active\\ PE_s\\ utilization\\tag{4}$$ 第一项反映了处理器架构将计算任务分配给PE单元的能力，参与计算的PE越多，那么PE的利用率就会越高，它由DNN的架构决定。例如通过网络映射的方法，如果硬件架构支持DNN模型不同计算层的结构，那么就可以让更多PE单元参与计算； 第二项反映了被激活的PE处理计算任务的效率，它与数据的带宽和存储器的延迟有关。 LatencyEnergy efficiencyPower consumptionHardware costFlexibilityScalabilityReference [1] V. Sze, Y. -H. Chen, T. -J. Yang and J. S. Emer, “How to Evaluate Deep Neural Network Processors: TOPS/W (Alone) Considered Harmful,” in IEEE Solid-State Circuits Magazine, vol. 12, no. 3, pp. 28-41, Summer 2020, doi: 10.1109/MSSC.2020.3002140.","link":"/2020/10/19/Metrics-DSA/"},{"title":"Project Review-3 本科毕业设计","text":"在神经网络中，存在着大量的参数，如果这些参数都用浮点数表示，将会消耗大量的硬件资源，这对于硬件资源有限的嵌入式平台是十分不友好的。在实际硬件实现中，往往采用将浮点数进行定点化处理，用定点数来表示浮点数。 数据的量化与截位数据的量化浮点数的定点化表示在目前的神经网络量化策略中，一般采用16位的定点数来表示数据，且对网络的精度损失比较低。因此，本项目中均采用16位的定点小数来表示输入值和参数。16位的定点数一共有3部分组成，1位符号位，4位整数位，11位小数位。小数位的位数也称量化系数（若小数位宽为0，则该定点数只有整数部分，此时也称为定点整数）。为了便于表示，本文将有符号定点数的表示方式定义如下：$$fix = mQn$$ 其中，$m$ 表示 定点数的位宽，$n$ 表示小数位宽，即量化系数。 那么，其浮点数的定点化公式转化如公式1所示：$$ fix = round(float\\times2^{n})\\tag{1}$$ 其python代码如下所示： 1234567891011121314# 浮点数量化为定点数(dec)def fp2fix(float_num, quant_width=11): fix_num = round(float_num * (2**quant_width)) return fix_num# 定点数(dec)转化为浮点数def fix2fp(fix_num, bit_width=16, quant_width=11): if (fix_num &gt;= 2**(bit_width-1)): float_num = (fix_num - 2**bit_width) / 2**quant_width else: float_num = fix_num / 2**quant_width return float_num 定点数的位数扩展在定点数的计算中，由于定点数的表示范围有限，对于$mQn$ 定点数的表示范围为$$-2^{m-n-1} \\sim 2^{m-n-1}- \\frac {1}{2^n}$$ 例如16$Q$11的数据表示范围为 $-16 \\sim 15.9995$。 在进行加法和乘法操作时，原先的位数已经无法正确表示计算后的结果，因此需要对数据进行位数的扩展。对于有符号定点数而言，在进行扩位时，整数部分需要进行符号位的扩展，小数部分则需要在末尾进行0补充。例如，将 $4Q2$ 定点数扩位成 $6Q3$ 的定点数，其具体扩位操作如下：$$\\begin{align}&amp;(4Q2)4’b 10.11=1\\times(-2^1)+0\\times2^0+1\\times2^{-1}+1*2^{-2}=-1.25\\\\ &amp;(6Q3)6’b 110.110 =1\\times(-2^2)+1\\times2^1+0\\times2^0+1\\times2^{-1}+1\\times2^{-2}+0\\times2^{-3}=-1.25\\\\\\end{align}$$ 由上式可知，数据经过扩位后数据大小并不发生变化。 数据的截位在定点数的加法和乘法运算中，为了保证数据的准确性，计算结果都是要进行扩位来保存的，但在实际中由于硬件资源有限，随着数据的频繁计算，不可能一直通过数据扩位来保存数据，因此在保证数据正确性的情况下，需要对数据进行截位处理。一种比较精确的处理方式是先对截位后的数据进行四舍五入处理，如果四舍五入过程中由于进位导致数据溢出，还需要对结果做饱和处理。 定点数的运算两个有符号定点数相加，两个加数一定要对齐小数点并进行符号位的拓展，为了保证数据不溢出，和的总位宽为加数的位宽+1。例如 $aQm$ 和 $bQn$ 相加（$a&gt;b,m&gt;n$），则两者的小数位宽要统一为m位，整数位宽统一为a位，且和的存储格式要用 $(a+1)Qm$存储。 两个有符号定点数相乘，为了保证积不溢出，积的总位宽为两个乘数位宽之和。例如 $aQm$ 和 $bQn$ 相乘, 则积的存储格式要用 $(a+b)Q(m+n)$ 存储。 四舍五入数据的截位主要依据定点数定义的小数位宽进行截位，但是把数据只进行简单的截取会使得结果不够精确，工程上一般采用通过判断符号位然后根据截掉的小数部分进行四舍五入进位的方式来处理。 对于正数，首先判断截取部分的最高位是否为1，若最高位为1，则需要进位加1；反之不需要进位。对于负数，进位判断正好相反，如果截取部分最高位为1以及其它位也有为1的情况，由于是负数，所以此时不需要进位。但与正数不同的是，负数不进位时需要加1；反之需要进位时不需要加1。其 verilog 代码如下： 1234// 小数位低位截位 -&gt; 四舍五入进位判断assign carry_bit = calc_out[2*dwidth-1]?(calc_out[qwidth-1] &amp; (|calc_out[qwidth-2:0])):calc_out[qwidth-1];assign dout_round = {calc_out[2*dwidth-1], calc_out[2*dwidth-1:qwidth]} + carry_bit; 饱和截位所谓的饱和截位是指如果计算结果超出了数据格式能表示的最大值，则用最大值来表示这个溢出的数据。同理，如果计算结果超过了数据的最小值，那么就用最小值来代替这个数据。其 verilog 代码如下： 1234// 整数位高位截位 -&gt; 溢出进行饱和操作assign dout = (dout_round[2*dwidth-qwidth:dwidth-1] == {(dwidth-qwidth+2){1'b0}} || dout_round[2*dwidth-qwidth:dwidth-1] == {(dwidth-qwidth+2){1'b1}})? dout_round[-1:0]:{dout_round[2*dwidth-qwidth],{(dwidth-1){!dout_round[2*dwidth-qwidtdwidthh]}}}; 卷积计算电路的计算结果对比为了验证计算结果的正确性，将之前转化好的定点数进行输入，经过卷积模块计算后，其最终的结果如表1所示。并将卷积电路计算得到的结果与Matlab软件计算结果进行比较，比较结果如下： fpga fix fpga fix-&gt;float result on matlab error 0xffe0 -0.01562500000 -0.01510682666 -0.0005 0xff96 -0.05175781250 -0.05172769939 -0.0000 0x0004 0.001953125000 0.002844710976 -0.0009 0xff91 -0.05419921875 -0.05379437370 -0.0004 0x001b 0.013183593750 0.014041541711 -0.0009 0xffda -0.01855468750 -0.01836803962 -0.0002 0xffaf -0.03955078125 -0.03915937634 -0.0004 0xffd3 -0.02197265625 -0.02108133655 -0.0009 0xffee -0.00878906250 -0.00842345750 -0.0003 表1：卷积计算电路的计算结果对比 由上表可知，该卷积电路计算结果基本正确，误差也在可接受范围之内，如果对数据精度有更高的要求，可以对定点数量化系数进行调整。","link":"/2020/04/23/Quantization/"},{"title":"Project Review-4 本科毕业设计","text":"该项目是我的本科毕业设计，主要设计了一种用于卷积神经网络推理加速的硬件加速器，并且使用 16 位定点数对网络参数进行量化，减小了硬件资源的消耗，实验基于ZYNQ（XC7Z010）硬件平台，完成了加速器的实现与测试，最终使用 MNIST 测试集的准确率为 98.42%,与Tensorflow 32 位浮点数计算结果仅相差 0.05%；与CPU(i5-6200U)推理速度相比，单张图片推理加速比达到了84.8倍，10000 张图片推理加速比达到了43.6倍。 项目回顾项目大纲首先是整个项目的总体回顾，整体工作的内容大纲如下： 实验平台搭建最终的实验平台是基于以下设计方案搭建，主要包括主要围绕 PC 与 ZYNQ、ARM 与 FPGA 这两组设备的通信展开，信号传输包括数据流和控制流。其中，PC 与 ZYNQ 之间的数据采用基于 UDP 协议的网口进行传输，而 ARM 与 ZYNQ 之间的数据采用片内的 AXI 总线和接口传输。 加速器的实验测试功能测试加速器的功能测试是基于图3搭建的软硬件协同工作平台，实验中使用 10000 张测试集图片进行测试，最终识别正确的图片为 9842 张，即测试集的识别准确率为 98.42%，而 Tensorflow 32 位浮点数计算的识别准确率为 98.47%，两者仅相差 0.05%。其 PC 终端显示的结果如图4所示： Tensorflow准确率 加速器准确率 准确率误差 98.47% 98.42% 0.05% 性能测试实验测试内容包括单张图片计算时间测试和10000张图片计算时间测试，并与笔记本CPU(i5-6200U)计算时间进行了对比。最终计算性能对比如下： 测试案例 CPU CNN加速器 加速比 单张图片/${\\mu}s$ 3985 47 84.8 10000张图片/$ms$ 21455 497 43.2 硬件资源消耗方面，该加速器的资源消耗情况如下表所示。其中 DSP 和LUT 资源消耗最多，DSP 资源利用率达到了 100%，这是因为在卷积计算单元中需要消耗较多的硬件乘法器，而加法树中的加法器硬件都是由LUT实现的。 Resource Utilization Available Utilization / % LUT 13619 17600 77.38 LUTRAM 678 6000 11.3 FF 12594 35200 35.78 BRAM 29 60 48.33 DSP 80 80 100 功耗方面，利用 Vivado 功耗估计器（XPE） 进行功耗的评估，最终得到加速器的总功耗为 1.991W。各部分的功耗分布如图5所示。其中，动态功耗包括时钟、信号、逻辑、片上存储、DSP 和 PS 端等资源消耗的功率，共计 1.863W，占整个加速器功耗的94%；静态功耗表示正常工作时晶体管漏电流消耗的功率，主要由芯片的设计本身决定，共计 0.129W，占整个加速器功耗的6%。加速器的功耗为总功耗减去PS侧的功耗，故实际加速器的功耗为0.458W。 图5：加速器功耗分布情况","link":"/2020/07/04/Review-final/"},{"title":"Project Review-1 本科毕业设计","text":"报告内容 卷积神经网络模型的搭建和训练 模型的参数获取及数据的定点化 基于 ZYNQ 的卷积神经网络加速器架构的设计 卷积核计算电路的设计、仿真以及计算结果对比 总结和计划 CNN 模型的搭建一种用于手写数字识别的卷积神经网络（Convolution Neural Network , 简称CNN ），参考了LeNet-5 模型，我对该模型的卷积核大小以及数量进行了调整，以便更好适配之后设计的硬件加速器，且两者的识别准确率相差不大。该网络模型包括输入层、卷积层、池化层、全连接层以及输出层。模型结构如图1所示： 图1：卷积神经网络的结构 其中输入图像像素大小为30x30 ,经过两组卷积层和池化层，最后连接两层全连接层。卷积层采用大小为3x3 的卷积核，步长为1；池化层采用的是2x2 区域的最大值池化；激活函数使用 ReLu 。使用 Tensorflow进行网络模型搭建，部分代码如下。在 Colab 上已经完成了模型的训练，该模型在10000 张验证集图片中准确率为98.13% 。部分源代码如下： 12345678910111213141516171819202122232425262728293031323334353637# 搭建网络结构class MNIST_CNN(Model): def __init__(self): super().__init__() self.conv1 = layers.Conv2D( filters=8, kernel_size=[3, 3], activation=tf.nn.relu, padding='valid', name='conv2d_1', input_shape=(30, 30, 1) ) self.pool1 = layers.MaxPool2D(pool_size=[2, 2], strides=2, name='max_pooling2d_1') self.conv2 = layers.Conv2D( filters=16, kernel_size=[3, 3], padding='valid', name='conv2d_2', activation=tf.nn.relu ) self.pool2 = layers.MaxPool2D(pool_size=[2, 2], strides=2, name='max_pooling2d_2') self.flatten = layers.Reshape(target_shape=(6 * 6 * 16,)) self.dense1 = layers.Dense(units=10) # self.dense2 = layers.Dense(units=10) def call(self, inputs): x = self.conv1(inputs) # [batch_size, 28, 28, 8] x = self.pool1(x) # [batch_size, 14, 14, 8] x = self.conv2(x) # [batch_size, 12, 12, 16] x = self.pool2(x) # [batch_size, 6, 6, 16] x = self.flatten(x) # [batch_size, 6 * 6 * 16] x = self.dense1(x) # [batch_size, 10] output = tf.nn.softmax(x) return output model = MNIST_CNN() 123# 验证准确率pred = 'Loss: {:.4f}, Accuracy: {:.4f}'print(pred.format(test_loss.result(),test_accuracy.result())) 模型的参数获取与数据的定点化利用 Tensorflow 的 checkpoint 函数可以对训练好的参数文件进行保存，由于在云端训练时采用的是32位的浮点数计算，故保存下来的参数是浮点数格式。但在 FPGA 上进行浮点数计算时，会消耗大量的硬件资源，可能无法在板上有限的硬件资源下完成加速器的硬件电路实现，因此需要将 浮点数做定点化处理，本次项目中使用的16位的定点数，包括1位符号位，4位整数位以及11位小数位，利用 python编写了数值转换的脚本，其部分结果如下表所示： 原始浮点数 定点数 定点数还原浮点数 误差值 0.10980392 0x00e1 0.10986328 -0.0000594 0.78039216 0x063e 0.78027344 0.0001187 0.33333333 0x02ab 0.33349609 -0.0001628 0.98823529 0x07e8 0.98828125 -0.0000460 0.97647059 0x07d0 0.97656250 -0.0000919 0.57254902 0x0495 0.57275391 -0.0002049 0.18823529 0x0182 0.18847656 -0.0002413 0.11372549 0x00e9 0.11376953 -0.0000440 0.33333333 0x02ab 0.33349609 -0.0001628 0.69803922 0x0596 0.69824219 -0.0002030 0.91372549 0x074f 0.91357422 0.00015130 表1：浮点数与定点数的比较 上述结果显示将浮点数转成定点数后，误差在$10^{-5}\\sim10^{-4}$范围内，因此采用将浮点数转化为定点数表示的方法是可行的。定点数的计算对FPGA硬件资源十分友好，且16位的定点数相比32位的浮点数可以大大减少了存储资源的占用和数据的加载速度。 基于ZYNQ 的卷积神经网络加速器架构的设计实现 CNN 加速器的整体实现流程主要通过 PC 端将手写数字图片发送至 ZYNQ 平台(包括 ARM(PL) 和 FPGA(PL) 即 ARM 接受图片数据并存放在 DDR 中，通过 AXI-GP 接口发送指令给 FPGA ，FPGA 接受到开始指令从 DDR 中读取图片数据和权值偏置参数，并完成卷积神经网络的计算，计算完成后反馈回 ARM ，最后再由 ARM 将计算结果发送回 PC 端并显示识别结果。其整体架构图如图2所示： 图2：加速器整体结构 CNN 加速器的架构主要包括输入缓存，加速器ip以及输出缓存。FPGA 的数据读入主要利用 AXI-HP 接口，从 DDR 中来获取图片数据和参数数据，然后加速器计算完成后再将结果写回 DDR 中，其加速器的架构图如图3所示： 图3：加速器ip外围架构 卷积核计算电路的设计首先，整个卷积计算电路单元模块化设计如图4所示。它包括数据缓存模块、卷积计算模块以及数据剔除模块。 图4：卷积计算单元 数据缓存模块由于输入图像是以数据流形式输入，而卷积计算通过滑窗对卷积核内的数据进行乘累加操作，因此我用三个移位寄存器组将输入数据流分成三行，对应于 3x3 的卷积核。同理，权重数据流也是类似操作，如图5所示： 图5：输入数据拆分操作 卷积计算单元卷积计算在硬件电路实现时可以有效利用其并行性，以 3x3 的卷积核为例，常见的硬件实现方案都是使用乘法矩阵和加法树进行计算，以下是两种常见的硬件实现方案： 图6：两种卷积计算电路方案 根据上述两种方案的设计结构，我采用了使用加法器更少的方案二，这样可以有效地节省 FPGA 的硬件资源，并且该方案的思路可以同时应用于池化层的设计。 数据剔除模块另外，在设计过程中发现，在卷积计算过程中，计算完成一行后会从下一行的第一列重新计算，但实际输入数据以数据流的形式输入，依次输入前后数据是连续的，数据流进行中会产生前一行的末尾数据与后一行的开头数据进行卷积的错误结果，因此需要将这些数据进行剔除。根据仿真波形的观察，错误数据的出现是有规律的，因此利用一个FIFO作为缓存，当数据到来的同时产生一个脉冲控制信号，控制FIFO的写入，通过设置剔除信号的占空比来剔除掉错误数据，将正确的数据写入FIFO中，然后在保证数据完整正确的情况下再从FIFO中读出。 图7：错误数据的形成 为了尽可能减少数据剔除中FIFO缓存的等待时间，假设输入图像大小为 ${n}\\times{n}$, 卷积核的大小为 ${3}\\times{3}$, 读入 $k$ 个数据后自动读出，则正确的输出数据个数为$$l = (n-2)^2\\tag{1}$$又 FIFO 读入速度为$$v1 = \\frac{n-2}{n}\\tag{2}$$FIFO 读出速度为$$v2 = 1\\tag{3}$$故为了保证数据不丢失, 则$$\\frac{k}{v2-v1} = \\frac{l}{v2}\\tag{4}$$因此$$k = \\frac{2(n-2)^2}{n}(向上取整)\\tag{5}$$其中 $k$ 值在 FIFO ip核的 prog full 阈值中设定。根据 prog full 信号,读入 $k$ 个数据后自动读出,这样既保证数据的不丢失，又可以减少缓存带来的等待时间。 卷积计算电路的仿真卷积计算电路仿真波形图如图8所示，其中，输入数据流为$\\left[\\begin{matrix}1 &amp; 2 &amp; \\cdots &amp; 5\\\\6 &amp; 7 &amp; \\cdots &amp; 10\\\\\\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots\\\\21 &amp; 22 &amp; \\cdots &amp; 25 \\end{matrix}\\right]$, 权值矩阵为$\\left[\\begin{matrix}1 &amp; 2 &amp; 3\\\\1 &amp; 2 &amp; 3\\\\1 &amp; 2 &amp; 3\\end{matrix}\\right]$。 图8：卷积计算电路的仿真图 总结和计划3月主要完成了CNN网络模型的训练和基本卷积电路计算单元的设计，并且完成了数据定点化的工作和卷积计算电路的仿真验证工作,原预期计划内容已基本完成。下一阶段打算完成池化层、全连接层等电路的设计和验证工作，完成整个卷积网络加速器的架构设计，并搭建好加速器外围数据写入和读出的硬件电路，进行上板测试。 参考文献 [1] Qiu, Jiantao, et al. “Going deeper with embedded fpga platform for convolutional neural network.” Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. 2016. [2] Ma, Yufei, et al. “Scalable and modularized RTL compilation of convolutional neural networks onto FPGA.” 2016 26th International Conference on Field Programmable Logic and Applications (FPL). IEEE, 2016. [3] Guo K, Zeng S, Yu J, et al. A survey of fpga-based neural network accelerator[J]. arXiv preprint arXiv:1712.08934, 2017. [4] Sankaradas, Murugan, et al. “A massively parallel coprocessor for convolutional neural networks.” 2009 20th IEEE International Conference on Application-specific Systems, Architectures and Processors. IEEE, 2009. [5] Zhai, Sheping, et al. “Design of Convolutional Neural Network Based on FPGA.” Journal of Physics: Conference Series. Vol. 1168. No. 6. IOP Publishing, 2019. [6] 秦华标,曹钦平.基于FPGA的卷积神经网络硬件加速器设计[J].电子与信息学报,2019,41(11):2599-2605. [7] 余子健. 基于FPGA的卷积神经网络加速器[D].浙江大学,2016.","link":"/2020/04/01/Review1/"},{"title":"Project Review-2 本科毕业设计","text":"报告内容 卷积计算单元的上板测试 池化电路的设计与仿真 卷积层架构的设计与仿真 卷积层电路的结果验证 总结和计划 卷积计算单元的上板测试卷积计算电路的测试电路主要包括由ARM、输入输出 FIFO、DMA 电路以及卷积加速电路。具体流程为ARM 端发送开始指令给卷积加速器，加速器从 DDR 中读取数据 ，利用FIFO 输入数据和权重，经卷积计算电路后将数据再写入 FIFO 中，并写回 DDR 中，写回后同时反馈给 ARM ， ARM 端再读取数据，通过串口打印出计算后的数据，测试电路的结果图如下： 图1：卷积计算单元测试电路效果图 池化电路的设计与仿真池化电路的设计与之前的卷积电路架构类似，也是利用移位寄存器进行滑窗操作，不同的是池化区域的大小为 2x2，且无权重参数，因此不需要乘法矩阵和加法树。本项目中的卷积神经网络采用的是最大值池化，因此只要将原先乘法器的位置换成最大值比较器即可，该池化电路设计原理图如下： 图2：池化电路设计图 由于在池化操作中，一般常采用步长为 2 的滑窗池化，所以相比于之前卷积计算中的步长为1，需要在原来的数据剔除电路中进行改进，将其进行一般化，即考虑步长，卷积核大小，输入特征图尺寸因素，得到一个通用的数据剔除电路。具体分析如下: 为了尽可能减少数据剔除中 FIFO 缓存的等待时间，假设输入图像大小为 ${n}\\times{n}$, 卷积核的大小为 ${m}\\times{m}$, 步长为 $u$, 读入 $k$ 个数据后自动读出，则正确的输出数据个数为$$l = (\\frac{n-m}{u}+1)^2\\tag{1}$$又 FIFO 读入速度为$$v1 = \\frac{\\frac{n-m}{u}+1}{un}\\tag{2}$$FIFO 读出速度为$$v2 = 1\\tag{3}$$故为了保证数据不丢失，则$$\\frac{k}{v2-v1} = \\frac{l}{v2}\\tag{4}$$因此$$k = (1-\\frac{n-m+u}{u^2n})({\\frac{n-m}{u}+1})^2(向上取整)\\tag{5}$$其中 $k$ 值在 FIFO ip 核的 prog full 阈值中设定。根据 prog full 信号，读入 $k$ 个数据后自动读出。这样既保证数据的不丢失，又可以减少缓存带来的等待时间。 卷积层电路的设计本项目中的网络模型结构有5层（除去输入输出层），包括卷积层1，池化层1，卷积层2，池化层2，全连接层。加速卷积层计算是加速该网络的核心，在卷积运算中，我们经常要对数据进行读写操作，因此需要对数据进行复用，根据卷积运算的并行方式，即有如下三种数据复用的方式： 图3：卷积运算中的数据复用方式 考虑到使用的FPGA开发版资源有限（DSP Slice数量为80），我设计了一个并行度为8的卷积层电路，即可同时进行8个卷积计算，我将其称之为卷积核列表（ConvPE List）。卷积核列表中包括8个卷积计算单元，每个计算单元需要9个乘法器，一共使用了72个DSP Slice，仅卷积层DSP利用率达90%。 同时，为了提高实现并行计算的效果，我将数据位宽统一为128（16x8）位。其中16是单个数据的位宽，8是卷积计算单元的个数。根据卷积神经网络的结构，卷积层含有两层，因此需要采用卷积核列表重复利用的方式，即一个电路供两次卷积层计算使用。通过上述分析，我设计了卷积层硬件电路，如图4所示： 图4：卷积层电路架构 根据网络结构，第一层为单通道特征图像输入，卷积核个数为8，则一次卷积核列表计算就可以完成卷积层1 的计算，之后再经过8个 Relu 和池化电路，得到8个通道的图像输出。同时，为了尽可能地减少在数据的读取加载中消耗太多时间，我将卷积层1的输出结果存放在了FPGA的片上RAM中（Block RAM），以方便后续计算层快速读取数据，具体硬件电路架构图如下： 图5：卷积层1电路架构 对于第二层网络，由于此时输入特征图像的通道数为8，卷积核个数为16，因为开发板的DSP资源有限，因此完成卷积层2的计算需要调用16次的卷积核列表，加法树，Relu和池化电路。数据的输入由之前存放在BRAM的图像缓存和权值缓存构成。具体的硬件电路架构图如下： 图6：卷积层2电路架构 卷积层电路的结果验证卷积层电路的验证与调试过程中，主要利用 Tensorflow 训练好的参数进行验证，打印输出各层的结果，并转换为16位的定点数进行比较。电路调试通过在仿真波形图中观察各个控制信号、输入输出数据的长度、数据同步性等。其最终仿真结果的波形图如下： 图7：卷积层计算仿真 将上述卷积加速器的计算结果与 Tensorflow 输出的结果进行对比，以其中一个输出通道为例（大小为 6x6)，对比结果如下： FPGA卷积加速器计算结果： $$\\left[\\begin{matrix} 0.0000 &amp; 0.0000 &amp; 0.1548 &amp; 0.0000 &amp; 1.4199 &amp; 1.6777\\\\0.2222 &amp; 1.2314 &amp; 0.6880 &amp; 0.5923 &amp; 2.1265 &amp; 1.4238\\\\1.2700 &amp; 1.7573 &amp; 0.0000 &amp; 2.1201 &amp; 2.2896 &amp; 0.4580\\\\1.6187 &amp; 1.3481 &amp; 1.9756 &amp; 3.1812 &amp; 2.0439 &amp; 0.4443\\\\1.2183 &amp; 0.8091 &amp; 0.7070 &amp; 1.6250 &amp; 0.9170 &amp; 0.0137\\\\0.0024 &amp; 0.0615 &amp; 0.6807 &amp; 1.8628 &amp; 0.3379 &amp; 0.0000\\end{matrix}\\right]$$ Tensorflow计算结果： $$\\left[\\begin{matrix} 0.0000 &amp; 0.0000 &amp; 0.1562 &amp; 0.0000 &amp; 1.4209 &amp; 1.6794\\\\0.2204 &amp; 1.2317 &amp; 0.6900 &amp; 0.5948 &amp; 2.1252 &amp; 1.4247\\\\1.2713 &amp; 1.7572 &amp; 0.0000 &amp; 2.1179 &amp; 2.2903 &amp; 0.0122\\\\1.6156 &amp; 1.3483 &amp; 1.9769 &amp; 3.1808 &amp; 2.0455 &amp; 0.0000\\\\1.2183 &amp; 0.8096 &amp; 0.7073 &amp; 1.6252 &amp; 0.9191 &amp; 0.0143\\\\0.0035 &amp; 0.0623 &amp; 0.7224 &amp; 1.8634 &amp; 0.3399 &amp; 0.0000\\end{matrix}\\right]$$ 两者误差结果： $$\\left[\\begin{matrix} 0.0000 &amp; 0.0000 &amp; -0.0015 &amp; 0.0000 &amp; -0.0010 &amp; -0.0017\\\\ 0.0018 &amp; -0.0003 &amp; -0.0020 &amp; -0.0026 &amp; 0.0013 &amp; -0.0009\\\\-0.0013 &amp; 0.0001 &amp; 0.0000 &amp; 0.0022 &amp; -0.0008 &amp; 0.4458\\\\ 0.0031 &amp; -0.0002 &amp; -0.0013 &amp; 0.0003 &amp; -0.0016 &amp; 0.4443\\\\-0.0000 &amp; -0.0005 &amp; -0.0002 &amp; -0.0002 &amp; -0.0021 &amp; -0.0007\\\\-0.0010 &amp; -0.0008 &amp; -0.0417 &amp; -0.0006 &amp; -0.0020 &amp; 0.0000\\end{matrix}\\right]$$ 由上述结果可知，该卷积层电路计算基本正确，考虑到卷积神经网络具有一定的鲁棒性，因此在误差不大的情况下，对最终的识别准确率影响不会很大。 总结和计划本月主要完成了加速器外围测试电路的搭建，数据剔除电路的通用化设计，卷积层电路的设计与仿真验证工作，对之前的电路进行了完善和改进，使卷积加速器更加通用化，以便可以适应各种不同的卷积计算，具有更广泛的应用前景。之后计划根据搭建好的测试平台，完成整个卷积加速器的上板测试，并与电脑 CPU 计算进行对比，若加速效果达到预期，则开始着手毕业论文的撰写，并进一步完善卷积神经网络加速器。","link":"/2020/04/22/Review2/"},{"title":"论文笔记","text":"Going Deeper with Embedded FPGA Platform for Convolutional Neural Network FPGA’16, February 21-23, 2016, Monterey, CA, USA Abstract CNN 模型中卷积层是以计算为中心，而全连接层是以内存为中心，即限制卷积层计算速度的关键在于大量的数据计算，而全连接层相比数据计算而言,限制速度的瓶颈在于数据读取的带宽 采用动态数据量化方法和一种卷积核的设计实现在整个 CNN 模型，可以提高 带宽 和 资源的利用率 Platform Device: Xilinx Zynq ZC706 CNN Model: VGG16-SVD Key Content 一种动态数据的量化方法 卷积核的3种并行 乘法器之间的并行 卷积核运算单元之间的并行 处理单元PE之间而的并行 数据搬移策略 Evaluation","link":"/2019/10/30/paper1/"},{"title":"Python 代码规范","text":"记录了Python编程过程中代码规范（PEP8）的问题，并且给出了相应的解决方法。 Python 代码规范 PEP 8 问题及解决 PEP 8: module level import not at top of file Solved：import不在文件的最上面，可能引用之前还有代码，把import引用放到文件的最上部就可以消除警告了。 PEP 8: expected 2 blank lines，found 0 Solved：期望上面有2个空白行，发现0个，添加两个空白行就可以了。 function name should be lowercase Solved：函数名改成小写。 PEP 8: indentation contains tabs Solved：缩进中有tab空格，推荐用四个空格缩进。 Indent expected Solved：意思是没有缩进，解析器报错了，添加缩进就可以了。 Unexpected indent Solved：不期望的缩进，重新添加符合规范的缩进或者Alt+Enter快捷键会提示你转化成规范的缩进。 PEP 8: missing whitespace around operator Solved：意思是操作符（‘=’，‘&lt;’等）前后丢失了空格，举个例子a=b会报警告，a = b正常。 PEP 8: no newline at end of file Solved：文件尾部没有新起一行，光标移到最后回车即可。 PEP 8: blank line at end of file Solved：文件最后多了一个空白行，只要有一个即可，删掉一个。 Shadows name ‘xxx’ from outer scope Solved：意思是‘xxx’在外部已经定义了，修改一下‘xxx’-&gt; ‘uuu’或者其他符合要求的修改都可。 PEP 8: block comment should start with ‘# ’ Solved：说的很清楚要以#加一个空格开始 PEP 8: inline comment should start with ‘# ’ Solved：注释信息单独放一行 PEP 8: multiple statements on one line (colon) Solved：多行语句写到一行了，Python3.0 好像不允许写到一行了，例如if x == 2: print(something)这样写就会有警告，必须要分两行。像下面这样12if x == 2: print(something) PEP 8: W291 trailing whitespace Solved：出现了多余的空格","link":"/2019/10/30/python_PEP8_Solutions/"},{"title":"AI core & RISC Architecture 课程笔记","text":"Week1：本周主要介绍了人工智能时代下计算机体系结构的现状、计算机的发展历史以及早期神经科学的发展与神经网络（多层感知机）的原理。 前言本系列主要是对 AI core and RISC Architecture 这门课程的学习总结，该课程是由复旦大学陈迟晓老师讲授，课程内容涉及人工智能时代下计算机体系结构的现状与发展，内容丰富，十分前沿，很值得学习。 课程链接 &gt;&gt;&gt; AI core and RISC Architecture 内容回顾课程简介 课程目的传统的通用计算平台已无法满足目前快速发展的AI算法的计算性能，因此需要一种新的硬件计算平台，本课程正是讲解人工智能时代背景下计算机体系结构的现状与发展，通过分析人工智能算法的计算原理，学习目前专用于人工智能计算的硬件体系结构与设计方法。 课程内容 AI-Core Architecture• Parallelism : Pipeline, Superscalar, SIMD• Architecture: ILP, GPU, CGRA, Neuromorphic• Optimization: data flow, quantization, sparsity• Computing-in-Memory Architecture RISC-V Architecture• Instruction Set Architecture• Single-Cycle RV32I Architecture• Pipelined RV32I and hazards 参考书籍 计算机的发展历史 Early Ages – Great Ideas The Birth of Computers Von-Neumann Architecture First CPU Chip（Intel 4004） 神经科学的早期发展与神经网络 Discovery of Neurons Neuron Structure and Modeling Visual Cortex in Cats Artificial Neural Networks","link":"/2020/07/05/week1/"},{"title":"AI core & RISC Architecture 课程笔记","text":"Week2：本周学习的知识主要包括：首先介绍了芯片工艺制造过程中缩放定律（Dennard Scaling）的变迁，在该变迁过程中，由于受到功耗的限制，产生了所谓的暗硅现象（Dark Silicon）；其次介绍了指令集架构（ISA）的概念和研究内容，详细阐述了CISC和RISC的区别，并介绍了一个评判ISA性能的理论公式；最后介绍了一个4条指令的极简RISC-V指令集的案例。 内容回顾缩放定律的变迁 摩尔定律（1965）的解释：Dennard Scaling 由美国科学家罗伯特唐纳德（DRAM的发明人）在1974年提出。指出随着芯片尺寸的进一步减小，内部的电场是保持不变的，即电场守恒缩放（Constant field Scaling）。因此随着芯片尺寸的虽小，假设尺寸变化比例为$S$。 Transistors Property Change ${\\Delta}$Quantity $S^2$ ${\\Delta}$Frequency $S$ ${\\Delta}$Capacity $\\frac{1}{S}$ ${\\Delta}{V_{dd}}^2$ $\\frac{1}{S^2}$ 那么根据功耗的表达式，芯片功耗的变化为$${\\Delta}Power={\\Delta}QFCV^2=1$$因此，在功耗可以保持不变的情况下，芯片的尺寸一直印证着摩尔定律在不断地缩小，性能也在不断地提高。 缩放定律的变迁：Post Dennard Scaling 原因：晶体管是有阈值电压的，电源电压是有一个极限的，它不能小于管子的阈值电压$V_t$，故电源电压不可能一直减小，最后会保持趋于一个定值。 发展：故传统Dennard Scaling不再适用，因此，电场守恒缩放（Constant field Scaling）逐步转变为电压守恒缩放（Coinstant Voltage Scaling），那么原先的功耗结果将发生变化，即$${\\Delta}Power={\\Delta}QFCV^2=S^2$$ 这就会造成随着芯片尺寸的减小，芯片的功耗将呈$S^2$倍的变化，导致芯片的功耗急剧增加。如图所示，当尺寸在0.1um的节点中，芯片单位面积产生的功耗已经可以和核电站的发热量相当，这使得了芯片的发展受到了自身功耗的很大限制，也产生了所谓的暗硅现象。 暗硅现象 暗硅现象的出现：由于芯片受到功耗的限制，对于多核处理器而言，他的每一个核并不能全部使用。因此在处理器工作时，有些处理器核是关闭，即所谓的“暗硅”（Dark Silicon）。但是芯片的工艺节点仍然在发展，如今已经到了7nm时代，这意味着我在相同面积的基础上能造更多的处理器核，但是实际工作的处理器核的利用率却越来越低。因此，为了避免以上现象的产生，工业界采用的解决方案是让处理器核处于“灰度状态”（Dim），即让处理器核在更低频率、更低功耗的状态下去工作，这样在消耗相同的功耗情况下，能同时运行的处理器核数可以增加，使得处理器核的利用率提高。这也是目前的处理器主频并没有提高太多的原因。 暗硅现象的发展：基于以上分析，由于Dark Silicon的存在，可以看出一个处理器芯片没有必要造更多的核数量，因为多余的核也是白白浪费的。然而，事实上却截然相反，Dark Silicon的存在反而使得在芯片上出现了更多的处理器核数量，这其中的原因主要如下： 商业行为：误导消费者处理器核数量越多，处理器性能越强； 更好地散热：芯片越大，有利于更好地散热； 制造成本：芯片设计成本较高，制造成本较低。处理器核数量越多，开核的自由度越高，有利于减少处理器的设计难度（增加处理器核之间的距离，帮助更好地散热，更好地优化能耗。例如处理器核通过类似于国际象棋的排布，其热量就不会过于集中，且特定的情况下，在部分极短的时间开启所有核满足性能需求，也不会造成热的积累）。 专用处理器的设计：对于目前的处理器而言，芯片功耗的降低比面积的降低更加重要。而使用专用的处理器核相比通用的处理器核能效比更高，产生的功耗也更加低，因此为了更好地提高处理器的性能，在多余核无用的情况下，不如将无用核的替换成专用的处理核，如现在SoC处理器中，一颗多核的CPU周边会搭配若干个专用的处理器核，如GPU、AI加速器、通信基带、视频编解码器等，这样当设备需要什么功能时直接调用相应的处理器核即可，这样将大大减少芯片的功耗，同时提高处理器的性能。因此，从一定程度上可以说，Dark Silicon的出现也为设计专业的处理器核奠定了一定的理论依据。 指令集架构 计算机体系结构研究的内容： 指令集架构（Instruction Set ArchitectureISA） 硬件组成（Hardware Organization） x86的64位指令集是由AMD发明的 RISC和CISC的重要区别：存储器的访问方式 RISC只能以load 和store指令访问存储器，而ALU能访问到寄存器列表，数据需要先从存储器加载到寄存中，是Load-Store类型； CISC的指令都可以访问到存储器，ALU也能直接访问储器，是Register-Memory类型 评估一个ISA性能的理论公式$$\\frac{Time}{Program}={\\frac{Instructions}{Program}}\\times{\\frac{Clock cycles}{Instructio}}\\times{\\frac{Time}{Clock cycles}}$$ RISC-V指令集 RISC-V指令集 指令集格式说明 一个4条指令的案例","link":"/2020/07/14/week2/"},{"title":"AI core & RISC Architecture 课程笔记","text":"Week3：本周学习的知识主要包括：Verilog的基本语法知识以及一个简单的RISC处理器的硬件组成，并使用Verilog描述了各个硬件模块的功能。 内容回顾Verilog语法的几点回顾 逻辑操作和位宽操作 logic operation: a &amp;&amp; b、a || b、!c Bit-wise operation: a&amp;b、a|b、~c 逻辑移位和算术移位 logic shift: a &gt;&gt; 1, a is unsigned arithmetic shift: a &gt;&gt;&gt; 1, a is signed 位宽划分 a[7-:4] = a[7:4] = a[4+:4] signed 补码 、unsigned 原码 由于之前上过Verilog的语法课，因此以上知识点只是整理了我认为比较重要的几处，更多的Verilog语法请参考以下链接 Verilog语法资料Verilog 有什么奇技淫巧？ 一个简单的RISC硬件组成麻雀虽小，五脏俱全。根据冯诺依曼理论，一个计算机包括存储器、运算器、控制器以及输入输出设备。该节介绍的极简RISC硬件也具有这些，当然不包括输入输出设备。下面将用Verilog去实现这些硬件电路，完成一个简单的RISC处理器。 存储器 (Memory) 寄存器列表（RF）123456789101112131415161718192021222324252627282930313233343536module regfile #( parameter REG_DATA_WIDTH = 4, parameter REG_ADDR_WIDTH = 16, parameter REG_NUMBER = 16)( input [REG_ADDR_WIDTH-1:0] rs1_addr, input [REG_ADDR_WIDTH-1:0] rs2_addr, input [REG_ADDR_WIDTH-1:0] rd_addr, input [REG_DATA_WIDTH-1:0] rd_data, input RegWEn, input clk,rst_n, output [REG_DATA_WIDTH-1:0] rs1_data, output [REG_DATA_WIDTH-1:0] rs2_data); reg [REG_DATA_WIDTH-1:0] rf [REG_NUMBER-1:0]; // reg0 is always equal to zero assign rs1_data = rs1_addr == 0? 0:rf[rs1_addr]; assign rs2_data = rs2_addr == 0? 0:rf[rs2_addr]; always@ (posedge clk or negedge rst_n) begin if (!rst_n) begin: REGFILE integer i; for (i=0; i&lt;REG_NUMBER; i=i+1) begin rf[i] &lt;= 0; end end else if (RegWEn &amp;&amp; rd_addr!=0) begin rf[rd_addr] &lt;= rd_data; end end endmodule 数据存储器（DCM）12345678910111213141516171819202122232425module dcmen #( parameter MEM_ADDR_WIDTH = 5, parameter MEM_DATA_WIDTH = 16, parameter MEM_NUMBER = 32)( input clk, input MemWEn, input [MEM_ADDR_WIDTH-1:0] addr, input [MEM_DATA_WIDTH-1:0] dataw, output [MEM_DATA_WIDTH-1:0] datar); reg [MEM_DATA_WIDTH-1:0]RAM [MEM_NUMBER-1:0]; always @(posedge clk) begin if (MemWEn) begin RAM[addr] &lt;= dataw; end end assign datar = RAM[addr] ; endmodule 指令存储器（ICM）1234567891011121314151617181920212223242526272829303132333435363738394041module icmem #( parameter PC_WIDTH = 16, parameter ISA_WIDTH = 16, parameter MEM_ADDR_WIDTH = 5, parameter MEM_DATA_WIDTH = 16, parameter MEM_NUMBER = 32)( input clk, input rst_n, input inst_wen, input [ISA_WIDTH-1:0] input_inst, output [ISA_WIDTH-1:0] current_inst); reg [PC_WIDTH-1:0] pc; always @(posedge clk or negedge rst_n) begin if (!rst_n) begin // reset pc &lt;= 0; end else begin pc &lt;= pc + 1; end end dcmen #( .MEM_ADDR_WIDTH(MEM_ADDR_WIDTH), .MEM_DATA_WIDTH(MEM_DATA_WIDTH), .MEM_NUMBER(MEM_NUMBER) ) inst_dcmen ( .clk (clk), .MemWEn (inst_wen), .addr (pc), .dataw (input_inst), .datar (current_inst) );endmodule 运算器 (ALU)123456789101112131415161718192021222324252627282930module MAC_ALU #( parameter REG_DATA_WIDTH = 16 )( input clk, input funct, input [REG_DATA_WIDTH-1:0] rs1,rs2, output [REG_DATA_WIDTH-1:0] rd); wire [REG_DATA_WIDTH-1:0] product; wire [REG_DATA_WIDTH-1:0] addend1,addend2; reg [REG_DATA_WIDTH-1:0] psum; assign product = $signed(rs1) * $signed(rs2); assign addend1 = funct? 0: product; assign addend2 = funct? rs2: psum; assign rd = addend1 + addend2; always @(posedge clk) begin if (funct) begin psum &lt;= 0; end else begin psum &lt;= rd; end end endmodule 关于Verilog语法中*的使用，可以参考以下链接 在Verilog中直接调用*实现乘法器，其延迟和占用资源如何？ 控制器 (IDU)1 极简指令集与其硬件组成 opcode 目标Reg 源寄存器/立即数 说明 Load rd rs+imm(5b) 在rst imm地址 -&gt; rd Store / rs(addr)/rs(data) rs(data) -&gt; Mem index=rs(地址) MOV rd imm(9b) 赋值 -&gt; rd MAC rd rs1/funct=1 乘加 MAC rd rs1/funct=1 乘加 注明：不同颜色的数据通路对应其相同颜色的指令 HomeWork","link":"/2020/08/21/week3/"},{"title":"AI core & RISC Architecture 课程笔记","text":"Week4：本周主要针对上周学习的RISC处理器的硬件组成，根据分析处理器性能评估公式，学习一些提高处理器的性能的方法，如流水线、定制化数据通路、超标量、乱序发射等。 内容回顾到目前为止，我们学习了一个基本RISC处理器的硬件组成，该硬件架构也称为单周期的处理器架构。单周期的处理器是在一个周期内完成取指令、指令解码、MAC计算、访问数据、写回数据这些步骤，那么有什么办法可以进一步提高处理器的速度呢？首先，我们来看一下前几周介绍的一个经典的评估指令集执行效率（处理器性能）的公式： $$\\frac{Time}{Program}={\\frac{Instructions}{Program}}\\times{\\frac{Clock\\ cycles}{Instruction}}\\times{\\frac{Time}{Clock\\ cycles}}\\tag{1}$$ 该公式的左端是程序执行的时间，可以反映处理器的性能。右端通过公式的巧妙拆分，变成了一个由程序包含的指令数量、每条指令执行的时钟周期数、每个周期需要的时间三部分组成的乘积式。对于人工智能芯片而言，一般往往用吞吐率（Throughput）来衡量AI芯片的性能，吞吐率的公式就是将上述公式1的分子分母进行互换，并将程序的内容换算成乘加操作（MAC）。转换后的公式如下： $$Throughput=\\frac{Operations}{Time}={\\frac{MAC}{Instructions}}\\times{\\frac{Instruction}{Clock\\ cycles}}\\times{\\frac{Clock\\ cycles}{Time}}\\tag{2}$$ 从等式的右端可以看出，由三部分组成，分别是每条指令包含的MAC数（MAC指令密度）、每个周期执行的指令数（一般认为“1”，因为每个周期读取一条指令）、每秒执行的周期数（主频）。对于AI计算而言，程序的内容主体为MAC指令，如果没有MAC指令，则完成一次乘累加运算需要两个指令，因此把一条MAC指令即一个乘法和一个加法操作等效为2次操作。 因此，吞吐率的公式可以简化为 $$Throughput\\ (MOPS)={MAC\\ Utilizatoin}\\times{\\ 1 \\ }\\times{f\\ (MHz)}\\tag{3}$$ 例如，对于之前提到的 Load Load MAC 指令，其MAC Utilization为 $\\frac{1}{3}$ ,假设 $f$ 为100MHz，一个MAC等效为2次操作，则该处理器的吞吐率约为66.67MOPS。基于以上分析和基础，下面我们将通过分析吞吐率公式的组成优化处理器的硬件架构。 流水线技术基本概念流水线(Pipeline)技术是指程序在执行时候多条指令重叠进行操作的一种准并行处理实现技术。通过在硬件架构中插入寄存器来切分各个执行阶段（取指令、指令解码、MAC计算、访问数据、写回数据），如图1所示： 对于采用流水线架构的处理器而言，其单条指令的执行时间几乎没有发生变化，依旧需要完成取指令、指令解码、MAC计算等步骤。但由于插入寄存器后每个步骤执行的时间变得很短，故处理器的主频可以得到很大的提升，假设每一个步骤运行的时间相等，那么采用5段流水线的频率可以提高5倍。当执行多条指令时，并行计算的优势便体现出来，通过随着源源不断地取指令，几乎每个CLK便可以完成一条指令。 同时，通过吞吐率的公式分析可知，采用流水线的技术实际上很大程度提高了处理器的频率，因此处理器的性能也会随着主频的提高呈同倍率的提升。 冒险现象（Hazard）然而，流水线技术并不是十分完美的，在指令并行执行的过程中，会产生一些“冒险”现象，如数据冒险（Data Hazard）、结构冒险（Structure Hazard）以及控制冒险（Control Hazard）等，带来各种硬件资源冲突，数据的读写顺序等问题。 结构冒险：所需的硬件正在为之前的指令工作； 数据冒险：需要等待之前的指令完成数据写入； 控制冒险：需要根据之前的指令决定接下来的行为； 这里以数据冒险为例，在上述案例中的计算指令中，指令的组成为Load、Load、MAC，那么其指令流水线的示意图如图3所示。 因此，对于Load、Load、MAC指令而言，在没有等到数据准备好时便执行MAC操作，其结果必然会出现错误。为了解决这个问题，最简单的方法便是在指令中插入空泡指令（Bubble），即一个空指令，为后续指令执行所需的数据争取一定的时间。其空指令插入示意图如图4所示： 可以看出，空指令的插入在一定程度上解决了数据冒险的问题，但他也导致了处理器的性能的损失，如原先的MAC Utilization为 $\\frac{1}{3}$ 现在变成了 $\\frac{1}{4}$, 吞吐率降低了25%。同时，我们也发现了流水线技术的使用，一方面给处理器性能带来了很大地提升，但另一方面也造成不少的麻烦和困扰，如下面两点： 1、处理器性能的损失，同时，对于流水线级数较深的处理器而言，流水线越深，其MAC Utilization的比例会降低地更加严重，使得处理器性能反而得不到很大地提升。 2、对于Branch/Jump等分支跳转指令而言，由于指令存在着不确定性，对处理器性能有一定的损失。以Branch指令为例，如果两个数相比较不满足条件，则需要跳转到其他指令，那么原先一大堆已获取的指令将全部抛弃重来，一定程度上造成了指令的浪费。 定制化流水线Forwarding对于上述由于data hazard造成MAC Untilization 大幅降低的问题，其实是有办法解决的，那就是采用定制化的流水线来改进原先的硬件结构。如图5所示，由于寄存器R3和R4的值在Cycle3和Cycle4便可以得到，因此我们可以提前将寄存器R3和R4的值直接连接到ALU中的计算单元，同时保持原先的数据通路，在Cycle4和Cycle5依旧写回到Regs File当中。在经典的计算机体系结构中，这种方法也叫做”forwarding”那么，在实际处理器中，我们该如何去实现这种硬件结构呢？对于某一段程序，我们在优化硬件电路时，最主要的是优化程序中最频繁出现的程序块，在本节讨论的范围中就是Load、Load、MAC指令，因此对于这部分优化，我们可以采用移位寄存器的方式，来提前保存从Regs File中读取的R3和R4，具体硬件电路如图6所示。 Aggressive Target到目前为止，我们用定制化的流水线解决了data hazard的问题，其MAC Utilization已经变到了原先的 $\\frac{1}{3}$,那么我们还能进一步提高MAC Utilization吗？答案当然是可以的。如图7所示，我们可以将指令集稍作修改，将第二条Load指令与MAC指令进行合并，作为一条新的指令Load-MAC，那么再Load之后的数据直接与上一次Load的数据进行运算，计算完成后再写回Regs File，那么此时MAC Utilization就变成了 $\\frac{1}{2}$。那么如果有Load-Load-MAC指令的话，其MAC Utilization就变成了 $100%$。此时，当Load MAC指令合并后，也引起了一个思考？因为第2周学习的有关RISC和CISC的区别时，一个重要的区分标准时ALU是否直接对存储器进行访问，那么当Load MAC变成一条指令后，此时的处理器是否还是RISC处理器？其实这两者都有自己的解释方式。 对于CISC而言，是把Load-MAC指令看作是一条MAC指令，那么此时MAC指令不再需要进行Load指令取数再进行MAC操作，也就是说通过MAC指令直接可以对存储器进行访问，因此相当于是CISC处理器的操作。 作为RISC而言，虽然Load-MAC指令是一条指令，但实际ALUM所需要的数据依旧是通过与Load指令相同的操作获取，这本质上和先Load指令再MAC指令没有区别，因此仍就是RISC处理器。 所谓的CISC也好，RISC也好，无非是站在不同角度来解释，因此是各有各的道理（我本人更加倾向于RISC的解释^-^），我们只要理解其数据存取的本质就行。 超标量处理器现在，我们再来看一下吞吐率的公式 $$Throughput\\ (MOPS)={MAC \\ Utilizatoin}\\times{\\ 1 \\ }\\times{f\\ (MHz)}$$ 与最原始的处理器吞吐率相比，通过定制化流水线后，MAC Utilization已经变为了 $\\frac{1}{2}$, $f$已经变为了原来的4倍，假设 $f$ 为100MHz，那么现在的吞吐率就变为了400MOPS。那么本节我们来看一下另一种优化吞吐率的方法。 超标量处理器的概念 标量处理器：每个周期执行一条指令的处理器 超标量处理器：每个周期执行多条指令的处理器，同时执行的多条指令使用的是不同硬件单元 因此，对于超标量处理器而言，如果多条指令之间存在着数据依赖关系，或者使用的是同一个硬件单元，那么就无法使用超标量技术。 超标量流水线（Superscalar Pipeline）以Load-MAC指令为例，使用超标量流水线架构在一个时钟周期内会执行多条指令，如图8所示。那么相比于原先的流水线架构而言，每个周期执行指令数为“2”，则处理器的吞吐量为 $$Throughput\\ (MOPS)=2\\times{\\frac{1}{2}}\\times{\\ 2 \\ }\\times{400\\ (MHz)}=800MOPS$$ 在具体的硬件结构实现中，由于两条指令均需要对Memory进行访问，因此在硬件架构中需要使用两个Memory单元，以满足需求。具体结构图如图9所示。 乱序发射（OoO）由于之前的指令都是顺序执行的，因此指令间并行时产生冒险现象时，后面的指令就需要等待。如果采用乱序发射技术，即将后面没有相关性的指令先执行，就可以尽可能地提高硬件利用率，因此它能够提高处理器执行指令的效率。 乱序发射(Out of Order)是指CPU采用了允许将多条指令不按程序规定的顺序分开发送给各相应电路单元处理的技术。 当然，乱序发射也可以解决其它问题，如：在现代高性能的处理器中，流水线级很长，由于存在L1、L2 catche，导致对Memory的访问是不确定的，这就导致了如Load指令与其他指令执行的时间会不一致，对Memory的访问时间有可能需要好几个周期。通过乱序发射技术，可以将处于空闲状态的指令提前执行，如下图10所示。这样提高了硬件的利用率，也就提高了指令的执行效率。","link":"/2020/08/29/week4/"}],"tags":[{"name":"CNN Accelerator","slug":"CNN-Accelerator","link":"/tags/CNN-Accelerator/"},{"name":"毕设项目","slug":"毕设项目","link":"/tags/%E6%AF%95%E8%AE%BE%E9%A1%B9%E7%9B%AE/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"课程笔记","slug":"课程笔记","link":"/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"}],"categories":[{"name":"Paper notes","slug":"Paper-notes","link":"/categories/Paper-notes/"},{"name":"Project","slug":"Project","link":"/categories/Project/"},{"name":"Problem sloved","slug":"Problem-sloved","link":"/categories/Problem-sloved/"},{"name":"Course","slug":"Course","link":"/categories/Course/"}]}