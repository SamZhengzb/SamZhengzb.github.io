<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Booster-2021 DAC System Design Contest</title>
      <link href="/2021/12/26/Booster-dac-sdc/"/>
      <url>/2021/12/26/Booster-dac-sdc/</url>
      
        <content type="html"><![CDATA[<p>DAC SDC（System Design Contest）的比赛任务是在Xilinx Ultra96v2平台上实现高速、高准确率且低功耗的飞行器高空目标检测深度学习算法。比赛所用的训练与测试数据集均由大疆公司提供，训练集包含95种目标共93K张图片，测试集包含52K张图片，最终将以检测精度（IoU）高、速度（FPS）快和功耗低为评判准则。</p><a id="more"></a> <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>读研期间，主要研究面向目标检测领域的加速器设计，包含两个项目：一个是2021 DAC System Design Contest 低功耗单目标检测系统设计竞赛，另一个是面向自动驾驶多目标检测场景的可配置深度学习加速器设计。这篇文章主要介绍一下2021 DAC System Design Contest 低功耗单目标检测系统设计竞赛。</p><h2 id="项目介绍"><a href="#项目介绍" class="headerlink" title="项目介绍"></a>项目介绍</h2><h3 id="任务介绍"><a href="#任务介绍" class="headerlink" title="任务介绍"></a>任务介绍</h3><p><img src="/2021/12/26/Booster-dac-sdc/dac.png" alt></p><p>DAC SDC（System Design Contest）的比赛任务是在Xilinx Ultra96v2平台上实现高速、高准确率且低功耗的飞行器高空目标检测深度学习算法。比赛所用的训练与测试数据集均由大疆公司提供，训练集包含95种目标共93K张图片，测试集包含52K张图片，最终将以检测精度（IoU）高、速度（FPS）快和功耗低为评判准则。</p><ul><li><strong>比赛官网</strong> <a href="https://dac-sdc-2021.groups.et.byu.net/doku.php">https://dac-sdc-2021.groups.et.byu.net/doku.php</a></li></ul><p><img src="/2021/12/26/Booster-dac-sdc/ultra96.png" alt></p><h3 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h3><p><img src="/2021/12/26/Booster-dac-sdc/model.png" alt></p><ul><li><strong>轻量化单目标检测网络</strong><ul><li>9层网络，输入图像尺寸为<strong>160x320x3</strong></li><li>backbone为类VGG结构； head部分采用YOLO系列结构；boxes select使用confidence最大值选择方法</li></ul></li><li><strong>低比特量化</strong><ul><li>权重使用int4，激活值使用uint4，BN参数使用int16</li><li>权重大小为<strong>0.296MB</strong>，BN大小为<strong>2.9KB</strong></li></ul></li><li><strong>精度优化</strong><ul><li>量化感知技术（PACT）、Mosaic数据增强、GIOU损失函数、Adam优化</li><li>模型训练得到 IoU为<strong>0.767</strong></li></ul></li></ul><h3 id="加速器设计"><a href="#加速器设计" class="headerlink" title="加速器设计"></a>加速器设计</h3><p>整个系统基于Xilinx Ultra96v2平台搭建，ARM（CPU）端运行装有PYNQ软件框架的Linux系统，CPU通过AXI-GP接口配置加速器和DMA，加速器的数据搬运依靠DMA走AXI-HP接口，使用的是AXI-Stream总线，数据位宽为32bit。加速器系统（Booster System）包含缓存模块、计算模块以及总线接口模块。</p><p><img src="/2021/12/26/Booster-dac-sdc/Booster_arch.png" alt></p><p><strong>缓存模块</strong>包含指令缓存、BN参数缓存、权重缓存以及图片的输入和计算结果的输出缓存。</p><p><strong>计算模块</strong>为加速器Booster的核心模块，主要由控制模块、2块特征图缓存缓存模块、PE模块、预处理模块、累加模块、BN ReLU模块（简称BR模块）、池化模块、NMS后处理模块组成。</p><h4 id="资源消耗"><a href="#资源消耗" class="headerlink" title="资源消耗"></a>资源消耗</h4><p><img src="/2021/12/26/Booster-dac-sdc/resource.png" alt></p><h4 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h4><table><thead><tr><th align="center"><strong>对比指标</strong></th><th align="center">DAC  SDC 2018 1st</th><th align="center">DAC  SDC 2019 1st</th><th align="center">DAC  SDC  2020  1st</th><th align="center"><strong>This</strong>    <strong>work</strong></th></tr></thead><tbody><tr><td align="center">Model</td><td align="center">SSD</td><td align="center">SkyNet</td><td align="center">UltraNet</td><td align="center">UltraNet*</td></tr><tr><td align="center">IOU</td><td align="center">0.624</td><td align="center">0.716</td><td align="center">0.656</td><td align="center">0.680</td></tr><tr><td align="center">FPS</td><td align="center">11.96</td><td align="center">25.05</td><td align="center">212.73</td><td align="center">158.00</td></tr><tr><td align="center">Power</td><td align="center">4.2  W</td><td align="center">7.26  W</td><td align="center">6.65  W</td><td align="center">4.62  W</td></tr><tr><td align="center">Energy Eff.</td><td align="center">2.85 Image/W</td><td align="center">3.45 Image/W</td><td align="center">31.99 Image/W</td><td align="center">34.20 Image/W</td></tr></tbody></table><ul><li>官方测试集，功耗为ZYNQ芯片功耗，*表示在原有基础上进行修改</li><li>加速器主频为215MHz，AXI Stream 总线位宽为32bit</li><li>比赛结果：<strong>SEU_Booster (9th/10th*)</strong>，<a href="https://dac-sdc-2021.groups.et.byu.net/doku.php?id=results">https://dac-sdc-2021.groups.et.byu.net/doku.php?id=results</a></li></ul><h4 id="Demo展示"><a href="#Demo展示" class="headerlink" title="Demo展示"></a>Demo展示</h4><p><img src="/2021/12/26/Booster-dac-sdc/show.gif" alt></p><p><img src="/2021/12/26/Booster-dac-sdc/real-build.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Project </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN Accelerator </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Booster-面向自动驾驶多目标检测场景的可配置加速器设计</title>
      <link href="/2021/12/26/Booster/"/>
      <url>/2021/12/26/Booster/</url>
      
        <content type="html"><![CDATA[<p>自动驾驶汽车应对在高速公路和主干道等高速行驶场景以及对各种突然事件需要做出及时的反应，如旁边车辆变道、前方车辆突发故障或前方行人突然出现等，检测系统要求具备较高的实时性，其中硬件加速平台是整个系统中最关键的部分。本项目设计的智能加速平台能够推理加速多种深度学习算法，帮助自动驾驶系统快速全面认知复杂路况、精准感知、预测车辆行驶过程中的交通情况，并且实时对周边环境做出准确的判断。此外，还可以应用在智能安防、智慧交通、工业生产等嵌入式领域。</p><a id="more"></a> <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>读研期间，主要研究面向目标检测领域的加速器设计，包含两个项目：一个是2021 DAC System Design Contest 低功耗单目标检测系统设计竞赛，另一个是面向自动驾驶多目标检测场景的可配置深度学习加速器设计。这篇文章主要介绍一下面向自动驾驶多目标检测场景的可配置深度学习加速器设计。</p><h2 id="项目介绍"><a href="#项目介绍" class="headerlink" title="项目介绍"></a>项目介绍</h2><h3 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h3><p><img src="/2021/12/26/Booster/Booster_arch.png" alt></p><p>整个系统基于Xilinx ZYNQ异构计算平台搭建，ARM（CPU）端运行装有PYNQ软件框架的Linux系统，CPU通过AXI-GP接口配置加速器和DMA，加速器的数据搬运依赖DMA走AXI-HP接口，使用的是AXI-Stream总线。加速器系统（Booster System）包含缓存模块、计算模块以及总线接口模块。</p><p><strong>缓存模块</strong>包含指令缓存、BN参数缓存、权重缓存以及图片的输入和计算结果的输出缓存。</p><p><strong>计算模块</strong>为加速器Booster的核心模块，主要由控制模块、2块特征图缓存缓存模块、PE模块、预处理模块、累加模块、BN ReLU模块（简称BR模块）、池化模块组成。</p><p><strong>加速器的主要特点</strong></p><ul><li>通用化的加速器架构（命名为Booster），加速器由缓存部分和计算部分构成，缓存模块用来配置网络模型的参数，用户可以通过向加速器系统的参数缓存和指令缓存配置网络模型的参数和描述模型结构的指令，便可直接部署不同CNN网络模型</li><li>高利用率的PE单元设计，可以实现标准卷积、深度卷积、点卷积3种不同类型的卷积计算</li><li>基于量化感知训练的软硬件协同设计（INT8，W8A8）</li><li>支持算子<ul><li>Standard Convolution、Depthwise Convolution、Pointwise Convolution</li><li>Max Pooling、Average Pooling</li><li>ReLU</li><li>Batch Normalization</li><li>Concat</li><li>Upsample</li><li>Branch</li><li>Round（四舍五入、四舍六入五成双）</li></ul></li></ul><h3 id="资源消耗情况"><a href="#资源消耗情况" class="headerlink" title="资源消耗情况"></a>资源消耗情况</h3><table><thead><tr><th align="center">Resource</th><th align="center">Utilization</th><th align="center">Available</th><th align="center">Utilization / %</th></tr></thead><tbody><tr><td align="center">LUT</td><td align="center">98193</td><td align="center">230400</td><td align="center">42.62</td></tr><tr><td align="center">LUTRAM</td><td align="center">28190</td><td align="center">101760</td><td align="center">27.70</td></tr><tr><td align="center">FF</td><td align="center">72421</td><td align="center">460800</td><td align="center">15.72</td></tr><tr><td align="center">BRAM</td><td align="center">156</td><td align="center">312</td><td align="center">50.00</td></tr><tr><td align="center">URAM</td><td align="center">96</td><td align="center">96</td><td align="center">100.00</td></tr><tr><td align="center">DSP</td><td align="center">625</td><td align="center">1728</td><td align="center">36.17</td></tr></tbody></table><h3 id="功耗情况"><a href="#功耗情况" class="headerlink" title="功耗情况"></a>功耗情况</h3><p><img src="/2021/12/26/Booster/power.png" alt></p><h3 id="Demo展示"><a href="#Demo展示" class="headerlink" title="Demo展示"></a>Demo展示</h3><h4 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h4><ul><li>模型结构：MobileNetV1 YoloV3-tiny</li><li>输入图片尺寸：640x320</li><li>训练数据集：BDD100k</li><li>训练效果</li></ul><table><thead><tr><th align="center">Class</th><th align="center"><a href="mailto:mAP@0.5">mAP@0.5</a>:0.95 （fp32）</th><th align="center"><a href="mailto:mAP@0.5">mAP@0.5</a>:0.95 （INT8）</th></tr></thead><tbody><tr><td align="center">all</td><td align="center">0.193</td><td align="center">0.185</td></tr><tr><td align="center">person</td><td align="center">0.153</td><td align="center">0.147</td></tr><tr><td align="center">rider</td><td align="center">0.0869</td><td align="center">0.0886</td></tr><tr><td align="center">car</td><td align="center">0.367</td><td align="center">0.349</td></tr><tr><td align="center">bus</td><td align="center">0.274</td><td align="center">0.259</td></tr><tr><td align="center">truck</td><td align="center">0.268</td><td align="center">0.251</td></tr><tr><td align="center">bike</td><td align="center">0.107</td><td align="center">0.105</td></tr><tr><td align="center">motor</td><td align="center">0.0938</td><td align="center">0.0949</td></tr></tbody></table><h4 id="场景一：市区街道场景应用"><a href="#场景一：市区街道场景应用" class="headerlink" title="场景一：市区街道场景应用"></a>场景一：市区街道场景应用</h4><ul><li>模型： MobileNetV1-YoloV3-tiny，输入尺寸640x320x3</li><li>平台：ZCU104 + PYNQ</li><li>场景搭建：OpenCV 读取Camera，依次进行图像Resize、加速器Inference、NMS处理和Box显示。</li></ul><p><img src="/2021/12/26/Booster/show1.gif" alt></p><p><img src="/2021/12/26/Booster/read_build1.png" alt></p><h4 id="场景二：公路场景应用"><a href="#场景二：公路场景应用" class="headerlink" title="场景二：公路场景应用"></a>场景二：公路场景应用</h4><ul><li>模型： MobileNetV1-YoloV3-tiny，输入尺寸640x320x3</li><li>平台：ZCU104 + PYNQ</li><li>场景搭建：OpenCV 读取Video，依次进行图像Resize、加速器Inference、NMS处理和Box显示。</li></ul><p><img src="/2021/12/26/Booster/show2.gif" alt></p><p><img src="/2021/12/26/Booster/real_build2.png" alt></p><h4 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h4><table><thead><tr><th align="center">Test Work</th><th align="center">FPS</th></tr></thead><tbody><tr><td align="center">Video Read Baseline</td><td align="center">67</td></tr><tr><td align="center">Camera Read Baseline</td><td align="center">14/90</td></tr><tr><td align="center">Booster</td><td align="center">52</td></tr><tr><td align="center">Booster+NMS(CPU)</td><td align="center">33</td></tr><tr><td align="center">Video+IMG Resize+Booster+NMS</td><td align="center">20</td></tr><tr><td align="center">Camera+IMG Resize+Booster+NMS</td><td align="center">10/24</td></tr></tbody></table><ul><li>Camera设备1为罗技的c270 (720p 30fps) , Camera Read Baseline测试为14fps</li><li>Camera设备2为 See3CAM_CU30_CHL_TC_BX  (1080p 60fps)，Camera Read Baseline测试为90fps</li><li>Booster 主频为 215MHz，AXI 总线传输位宽为 32bit</li></ul>]]></content>
      
      
      <categories>
          
          <category> Project </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN Accelerator </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Key Metrics and Design Objectives for DNN Accelerator</title>
      <link href="/2020/10/19/Metrics-DSA/"/>
      <url>/2020/10/19/Metrics-DSA/</url>
      
        <content type="html"><![CDATA[<p>本文详细介绍了评估DNN加速器时需要考虑的指标和影响因素，并讨论了在设计DNN加速器时应该如何做trade-off。</p><a id="more"></a><h2 id="Key-Metrics-and-Design-Objectives"><a href="#Key-Metrics-and-Design-Objectives" class="headerlink" title="Key Metrics and Design Objectives"></a>Key Metrics and Design Objectives</h2><p>目前，深度学习加速器不断涌现，各种“<em>XPU</em>”深度学习加速器都有自己独特的架构优化，那么我们应该如何评估一款加速器的性能以及比较不同DNN加速器之间的好坏呢？或者说在设计一款DNN加速器时需要考虑到哪些因素呢？通常，一款通用处理器的性能我们往往用$FLOPS/W$或者$TOPS/W$来表示，但只关注加速器的性能是不够全面的，我们需要一个全面并且客观的体系来评估加速器各方面的特点，<strong>因此研究DNN加速器的评估指标，制定相应的评估系统就显得十分重要</strong>。在DNN加速器的设计过程中，加速器设计的好坏与许多因素有关，主要包括</p><ul><li>精度（accuracy）</li><li>吞吐率（throughput）</li><li>延迟（latency）</li><li>能效（energy efficiency）</li><li>功耗（power）</li><li>成本（cost）</li><li>灵活性（flexibility）</li><li>可拓展性（scalability）</li></ul><p>下面我们将详细介绍这些因素是如何影响DNN加速器的性能和能耗，以及讨论我们在设计DNN加速器时应该如何做折中考虑（trade-off）。</p><h3 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h3><p><strong>准确率（accuracy）是对给定任务下识别结果质量的评判</strong>。在不用任务下，准确率的定义是不一样的。例如，在图像分类任务中，准确率的定义是正确分类图片的百分比；在目标检测任务中，准确率是各类AP值的平均值（<a href="https://www.zhihu.com/question/53405779">mean average precision，mAP</a>）。</p><p><strong>准确率的影响因素主要包括任务难度和数据集</strong>。例如，在<em>ImageNet</em>上的分类任务比在<em>MNIST</em>上的分类任务要难很多，因此准确率会低很多；目标检测任务和语意分割任务会比图像分类任务要复杂很多，因此准确率也会低一些。</p><h3 id="Throughput"><a href="#Throughput" class="headerlink" title="Throughput"></a>Throughput</h3><p><strong>吞吐率（Throughput）表示在给定时间下处理器能够处理数据量的大小或执行任务的数量</strong>。即表示在单位时间下能够执行计算的数量。通常，吞吐率的表达式为：<br>$$Throughput=\frac{operations}{second}$$</p><p>在深度学习推理计算过程中，其执行的操作数等效为推理（inference）的次数，因此，吞吐率可以表示为：<br>$$Throughput=\frac{inferences}{second}\tag{1}$$</p><p>高吞吐率是我们想要的，因为在一些实时任务下，对数据的实时处理要求较高，高吞吐率意味着在单位时间内我们能够处理的数据更多，那么我们就可以实现高帧率或者高性能处理数据，这在无人驾驶、金融、国防安全等领域有着重要的意义。</p><p>那么我们该如何设计高吞吐率的处理器呢？我们将吞吐率的公式进行分解，具体如下：<br>$$\frac{inferences}{second}={\frac{operations}{second}}\times{\frac{1}{\frac{operations}{inference}}}\tag{2}$$</p><p>其中，<em>operations per second</em> 由DNN加速器和DNN模型决定；<em>operations per inference</em> 由DNN模型决定；</p><blockquote><p><strong>attention</strong>：式中这两个<em>operations</em>是不一样的。细心的读者可能会发现，如果将两个<em>operations</em>等同消掉，将右边式子还原，分子就会变成<em>inference</em>，而左边分子是<em>inferences</em>，那么两边就不相等了。<strong>为什么会这样</strong>？这是因为前面<em>operations per second</em>中的<em>operations</em>是包含倍数因子的，和<em>inferences</em>类似。举个例子，<em>inferences</em>这个值可以改变的，一般是大于1，而<em>inference</em>是单次推理（即为1），因此<em>inferences</em>对<em>inference</em>有个倍数关系。同理，<em>operations per inference</em>中的<em>operations</em>是由模型确定的，如果模型确定就是一个常量，但是前面<em>operations per second</em>中的<em>operations</em>肯定不是一个常量，它是受硬件设计影响的，两者存在一个倍数关系，因此综上所述，式中这两个<em>operations</em>是不相等的，不能等同处理。</p></blockquote><p>作为DNN加速器的硬件设计者，我们主要关注 <em>operations per second</em> 这个项，它可以进一步分解为：<br>$$\frac{operations}{second}=(\frac{1}{\frac{cycles}{operation}}\times{\frac{cycles}{second}})\times PE_s\times PE_s\ utilization\tag{3}$$</p><p>其中，第一项（括号）反映的是单个PE的峰值吞吐率，它由 <em>cycles per operation</em> 和 <em>cycles per second</em> 组成；第二项是PE单元的数量，反映计算的并行程度；第三项是PE单元的利用率，它由DNN加速器的架构设计决定。</p><ul><li>对于第一项而言，<em>cycles per operation</em> 是由PE单元中MAC的设计决定的，如采用流水线结构的MAC每个 <em>operation</em> 需要的 <em>cycles</em> 就会很低，<em>cycles per second</em> 即处理器主频$f$(MHz)，可以通过优化电路的关键路径来提高。</li><li>对于第二项而言，PEs的数量提升，会有更多的PE单元同时参与计算，会提高整个加速器的吞吐率。但是在整个芯片面积给定和单个PE面积不变的情况下，PE单元的数量增加通过增加PE阵列的面积，那么会造成片上存储面积的减少，这会进一步影响PE单元的利用率，从而影响到整个加速器的吞吐率。</li></ul><p>对于第三项，我们单独来讨论，因为这涉及到DNN加速器的架构，与我们的设计有很大关系，也是我们关注的重点。理想情况下，当PE的利用率达到100%时，尽可能地提高PE的数量和单个PE的峰值吞吐率，可以实现吞吐率的最大值。但是事实上，PE的利用率并不能完全达到100%,实际的吞吐率也取决于实际PE的利用率。对于PE的利用率，它可以分解为：<br>$$PE_s\ utilization=\frac{number\ of\ active PE_s}{number\ of\ PE_s}\times active\ PE_s\ utilization\tag{4}$$</p><ul><li>第一项反映了处理器架构将计算任务分配给PE单元的能力，参与计算的PE越多，那么PE的利用率就会越高，它由DNN的架构决定。例如通过网络映射的方法，如果硬件架构支持DNN模型不同计算层的结构，那么就可以让更多PE单元参与计算；</li><li>第二项反映了被激活的PE处理计算任务的效率，它与数据的带宽和存储器的延迟有关。</li></ul><h3 id="Latency"><a href="#Latency" class="headerlink" title="Latency"></a>Latency</h3><h3 id="Energy-efficiency"><a href="#Energy-efficiency" class="headerlink" title="Energy efficiency"></a>Energy efficiency</h3><h3 id="Power-consumption"><a href="#Power-consumption" class="headerlink" title="Power consumption"></a>Power consumption</h3><h3 id="Hardware-cost"><a href="#Hardware-cost" class="headerlink" title="Hardware cost"></a>Hardware cost</h3><h3 id="Flexibility"><a href="#Flexibility" class="headerlink" title="Flexibility"></a>Flexibility</h3><h3 id="Scalability"><a href="#Scalability" class="headerlink" title="Scalability"></a>Scalability</h3><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><blockquote><p>[1] V. Sze, Y. -H. Chen, T. -J. Yang and J. S. Emer, “How to Evaluate Deep Neural Network Processors: TOPS/W (Alone) Considered Harmful,” in IEEE Solid-State Circuits Magazine, vol. 12, no. 3, pp. 28-41, Summer 2020, doi: 10.1109/MSSC.2020.3002140.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN Accelerator </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>流水线的那些事儿</title>
      <link href="/2020/08/29/week4/"/>
      <url>/2020/08/29/week4/</url>
      
        <content type="html"><![CDATA[<p><strong>Week4</strong>：本周主要针对上周学习的RISC处理器的硬件组成，根据分析处理器性能评估公式，学习一些提高处理器的性能的方法，如流水线、定制化数据通路、超标量、乱序发射等。<a id="more"></a></p><h2 id="内容回顾"><a href="#内容回顾" class="headerlink" title="内容回顾"></a>内容回顾</h2><p>到目前为止，我们学习了一个基本RISC处理器的硬件组成，该硬件架构也称为单周期的处理器架构。单周期的处理器是在一个周期内完成取指令、指令解码、MAC计算、访问数据、写回数据这些步骤，那么有什么办法可以进一步提高处理器的速度呢？首先，我们来看一下前几周介绍的一个经典的评估指令集执行效率（处理器性能）的公式：<br>  $$\frac{Time}{Program}={\frac{Instructions}{Program}}\times{\frac{Clock\ cycles}{Instruction}}\times{\frac{Time}{Clock\ cycles}}\tag{1}$$</p><p>该公式的左端是程序执行的时间，可以反映处理器的性能。右端通过公式的巧妙拆分，变成了一个由程序包含的指令数量、每条指令执行的时钟周期数、每个周期需要的时间三部分组成的乘积式。对于人工智能芯片而言，一般往往用吞吐率（<strong>Throughput</strong>）来衡量AI芯片的性能，吞吐率的公式就是将上述公式1的分子分母进行互换，并将程序的内容换算成乘加操作（MAC）。转换后的公式如下：<br>  $$Throughput=\frac{Operations}{Time}={\frac{MAC}{Instructions}}\times{\frac{Instruction}{Clock\ cycles}}\times{\frac{Clock\ cycles}{Time}}\tag{2}$$</p><p>从等式的右端可以看出，由三部分组成，分别是每条指令包含的MAC数（MAC指令密度）、每个周期执行的指令数（一般认为“1”，因为每个周期读取一条指令）、每秒执行的周期数（主频）。对于AI计算而言，程序的内容主体为MAC指令，如果没有MAC指令，则完成一次乘累加运算需要两个指令，因此把一条MAC指令即一个乘法和一个加法操作等效为2次操作。</p><p>因此，吞吐率的公式可以简化为<br>  $$Throughput\ (MOPS)={MAC\ Utilizatoin}\times{\ 1 \ }\times{f\ (MHz)}\tag{3}$$</p><p>例如，对于之前提到的 <strong>Load Load MAC</strong> 指令，其MAC Utilization为 $\frac{1}{3}$ ,假设 $f$ 为100MHz，一个MAC等效为2次操作，则该处理器的吞吐率约为66.67MOPS。基于以上分析和基础，下面我们将通过分析吞吐率公式的组成优化处理器的硬件架构。</p><h3 id="流水线技术"><a href="#流水线技术" class="headerlink" title="流水线技术"></a>流水线技术</h3><h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><p>流水线(Pipeline)技术是指程序在执行时候多条指令重叠进行操作的一种准并行处理实现技术。通过在硬件架构中插入寄存器来切分各个执行阶段（取指令、指令解码、MAC计算、访问数据、写回数据），如图1所示：<br><img src="/2020/08/29/week4/pipeline_hw.png" alt="图1：采用流水线的硬件架构"></p><p>对于采用流水线架构的处理器而言，其单条指令的执行时间几乎没有发生变化，依旧需要完成取指令、指令解码、MAC计算等步骤。但由于插入寄存器后每个步骤执行的时间变得很短，故处理器的主频可以得到很大的提升，假设每一个步骤运行的时间相等，那么采用5段流水线的频率可以提高5倍。当执行多条指令时，并行计算的优势便体现出来，通过随着源源不断地取指令，几乎每个CLK便可以完成一条指令。<br><img src="/2020/08/29/week4/pipeline_plot.png" alt="图2：采用流水线的指令执行示意图"></p><p>同时，<strong>通过吞吐率的公式分析可知，采用流水线的技术实际上很大程度提高了处理器的频率，因此处理器的性能也会随着主频的提高呈同倍率的提升。</strong></p><h4 id="冒险现象（Hazard）"><a href="#冒险现象（Hazard）" class="headerlink" title="冒险现象（Hazard）"></a>冒险现象（Hazard）</h4><p>然而，流水线技术并不是十分完美的，在指令并行执行的过程中，会产生一些“冒险”现象，如数据冒险（Data Hazard）、结构冒险（Structure Hazard）以及控制冒险（Control Hazard）等，带来各种硬件资源冲突，数据的读写顺序等问题。</p><ul><li>结构冒险：所需的硬件正在为之前的指令工作；</li><li>数据冒险：需要等待之前的指令完成数据写入；</li><li>控制冒险：需要根据之前的指令决定接下来的行为；</li></ul><p>这里以数据冒险为例，在上述案例中的计算指令中，指令的组成为Load、Load、MAC，那么其指令流水线的示意图如图3所示。<br><img src="/2020/08/29/week4/data_hazard.png" alt="图3：数据冒险的示意图"></p><p>因此，对于Load、Load、MAC指令而言，在没有等到数据准备好时便执行MAC操作，其结果必然会出现错误。为了解决这个问题，最简单的方法便是在指令中插入空泡指令（Bubble），即一个空指令，为后续指令执行所需的数据争取一定的时间。其空指令插入示意图如图4所示：<br><img src="/2020/08/29/week4/bubble.png" alt="图4：空指令插入的示意图"></p><p>可以看出，空指令的插入在一定程度上解决了数据冒险的问题，但他也导致了处理器的性能的损失，如原先的MAC Utilization为 $\frac{1}{3}$ 现在变成了 $\frac{1}{4}$, 吞吐率降低了25%。同时，我们也发现了流水线技术的使用，一方面给处理器性能带来了很大地提升，但另一方面也造成不少的麻烦和困扰，如下面两点：</p><ul><li>1、处理器性能的损失，同时，对于流水线级数较深的处理器而言，流水线越深，其MAC Utilization的比例会降低地更加严重，使得处理器性能反而得不到很大地提升。</li><li>2、对于Branch/Jump等分支跳转指令而言，由于指令存在着不确定性，对处理器性能有一定的损失。以Branch指令为例，如果两个数相比较不满足条件，则需要跳转到其他指令，那么原先一大堆已获取的指令将全部抛弃重来，一定程度上造成了指令的浪费。</li></ul><h3 id="定制化流水线"><a href="#定制化流水线" class="headerlink" title="定制化流水线"></a>定制化流水线</h3><h4 id="Forwarding"><a href="#Forwarding" class="headerlink" title="Forwarding"></a>Forwarding</h4><p>对于上述由于data hazard造成MAC Untilization 大幅降低的问题，其实是有办法解决的，那就是采用定制化的流水线来改进原先的硬件结构。<br>如图5所示，由于寄存器R3和R4的值在Cycle3和Cycle4便可以得到，因此我们可以提前将寄存器R3和R4的值直接连接到ALU中的计算单元，同时保持原先的数据通路，在Cycle4和Cycle5依旧写回到Regs File当中。在经典的计算机体系结构中，这种方法也叫做”<strong>forwarding</strong>”<br><img src="/2020/08/29/week4/forwarding.png" alt="图5：定制化流水线示意图"><br>那么，在实际处理器中，我们该如何去实现这种硬件结构呢？对于某一段程序，我们在优化硬件电路时，最主要的是优化程序中最频繁出现的程序块，在本节讨论的范围中就是Load、Load、MAC指令，因此对于这部分优化，我们可以采用移位寄存器的方式，来提前保存从Regs File中读取的R3和R4，具体硬件电路如图6所示。<br><img src="/2020/08/29/week4/forwarding_hw.png" alt="图6：定制化流水线对应的数据通路"></p><h4 id="Aggressive-Target"><a href="#Aggressive-Target" class="headerlink" title="Aggressive Target"></a>Aggressive Target</h4><p>到目前为止，我们用定制化的流水线解决了data hazard的问题，其MAC Utilization已经变到了原先的 $\frac{1}{3}$,那么我们还能进一步提高MAC Utilization吗？答案当然是可以的。如图7所示，我们可以将指令集稍作修改，将第二条Load指令与MAC指令进行合并，作为一条新的指令Load-MAC，那么再Load之后的数据直接与上一次Load的数据进行运算，计算完成后再写回Regs File，那么此时MAC Utilization就变成了 $\frac{1}{2}$。那么如果有Load-Load-MAC指令的话，其MAC Utilization就变成了 $100%$。<br><img src="/2020/08/29/week4/pipeline2.png" alt="图7：Load-MAC指令示意图"><br>此时，当Load MAC指令合并后，也引起了一个思考？因为第2周学习的有关RISC和CISC的区别时，一个重要的区分标准时ALU是否直接对存储器进行访问，那么当Load MAC变成一条指令后，此时的处理器是否还是RISC处理器？其实这两者都有自己的解释方式。</p><ul><li>对于CISC而言，是把Load-MAC指令看作是一条MAC指令，那么此时MAC指令不再需要进行Load指令取数再进行MAC操作，也就是说通过MAC指令直接可以对存储器进行访问，因此相当于是CISC处理器的操作。</li><li>作为RISC而言，虽然Load-MAC指令是一条指令，但实际ALUM所需要的数据依旧是通过与Load指令相同的操作获取，这本质上和先Load指令再MAC指令没有区别，因此仍就是RISC处理器。</li></ul><p>所谓的CISC也好，RISC也好，无非是站在不同角度来解释，因此是各有各的道理（我本人更加倾向于RISC的解释^-^），我们只要理解其数据存取的本质就行。</p><h3 id="超标量处理器"><a href="#超标量处理器" class="headerlink" title="超标量处理器"></a>超标量处理器</h3><p>现在，我们再来看一下吞吐率的公式<br>  $$Throughput\ (MOPS)={MAC \ Utilizatoin}\times{\ 1 \ }\times{f\ (MHz)}$$</p><p>与最原始的处理器吞吐率相比，通过定制化流水线后，MAC Utilization已经变为了 $\frac{1}{2}$, $f$已经变为了原来的4倍，假设 $f$ 为100MHz，那么现在的吞吐率就变为了400MOPS。那么本节我们来看一下另一种优化吞吐率的方法。</p><h4 id="超标量处理器的概念"><a href="#超标量处理器的概念" class="headerlink" title="超标量处理器的概念"></a>超标量处理器的概念</h4><ul><li>标量处理器：每个周期执行一条指令的处理器</li><li>超标量处理器：每个周期执行多条指令的处理器，同时执行的多条指令使用的是不同硬件单元</li></ul><p>因此，对于超标量处理器而言，如果多条指令之间存在着数据依赖关系，或者使用的是同一个硬件单元，那么就无法使用超标量技术。</p><h4 id="超标量流水线（Superscalar-Pipeline）"><a href="#超标量流水线（Superscalar-Pipeline）" class="headerlink" title="超标量流水线（Superscalar Pipeline）"></a>超标量流水线（Superscalar Pipeline）</h4><p>以Load-MAC指令为例，使用超标量流水线架构在一个时钟周期内会执行多条指令，如图8所示。<br><img src="/2020/08/29/week4/superscalar.png" alt="图8：超标量流水线示意图"><br>那么相比于原先的流水线架构而言，每个周期执行指令数为“2”，则处理器的吞吐量为<br>  $$Throughput\ (MOPS)=2\times{\frac{1}{2}}\times{\ 2 \ }\times{400\ (MHz)}=800MOPS$$</p><p>在具体的硬件结构实现中，由于两条指令均需要对Memory进行访问，因此在硬件架构中需要使用两个Memory单元，以满足需求。具体结构图如图9所示。<br><img src="/2020/08/29/week4/superscalar_hw.png" alt="图9：超标量流水线示意图"></p><h4 id="乱序发射（OoO）"><a href="#乱序发射（OoO）" class="headerlink" title="乱序发射（OoO）"></a>乱序发射（OoO）</h4><p>由于之前的指令都是顺序执行的，因此指令间并行时产生冒险现象时，后面的指令就需要等待。如果采用乱序发射技术，即将后面没有相关性的指令先执行，就可以尽可能地提高硬件利用率，因此它能够提高处理器执行指令的效率。</p><blockquote><p>乱序发射(Out of Order)是指CPU采用了允许将多条指令不按程序规定的顺序分开发送给各相应电路单元处理的技术。</p></blockquote><p>当然，乱序发射也可以解决其它问题，如：在现代高性能的处理器中，流水线级很长，由于存在L1、L2 catche，导致对Memory的访问是不确定的，这就导致了如Load指令与其他指令执行的时间会不一致，对Memory的访问时间有可能需要好几个周期。通过乱序发射技术，可以将处于空闲状态的指令提前执行，如下图10所示。这样提高了硬件的利用率，也就提高了指令的执行效率。</p><p><img src="/2020/08/29/week4/OoO.png" alt="图10：乱序发射"></p>]]></content>
      
      
      <categories>
          
          <category> Course </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 课程笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Verilog and RISC hardware organization</title>
      <link href="/2020/08/21/week3/"/>
      <url>/2020/08/21/week3/</url>
      
        <content type="html"><![CDATA[<p><strong>Week3</strong>：本周学习的知识主要包括：Verilog的基本语法知识以及一个简单的RISC处理器的硬件组成，并使用Verilog描述了各个硬件模块的功能。<a id="more"></a></p><h2 id="内容回顾"><a href="#内容回顾" class="headerlink" title="内容回顾"></a>内容回顾</h2><h3 id="Verilog语法的几点回顾"><a href="#Verilog语法的几点回顾" class="headerlink" title="Verilog语法的几点回顾"></a>Verilog语法的几点回顾</h3><ul><li>逻辑操作和位宽操作<ul><li>logic operation: a &amp;&amp; b、a || b、!c</li><li>Bit-wise operation:  a&amp;b、a|b、~c</li></ul></li><li>逻辑移位和算术移位<ul><li>logic shift:  a &gt;&gt; 1, a is unsigned  </li><li>arithmetic shift:  a &gt;&gt;&gt; 1, a is signed </li></ul></li><li>位宽划分<ul><li>a[7-:4] = a[7:4] = a[4+:4]</li></ul></li><li>signed 补码 、unsigned 原码</li></ul><p>由于之前上过Verilog的语法课，因此以上知识点只是整理了我认为比较重要的几处，更多的Verilog语法请参考以下链接</p><blockquote><p><a href="https://arch.cihlab.top/verilog_training.pdf">Verilog语法资料</a><br><a href="https://www.zhihu.com/question/54815861/answer/1138376234">Verilog 有什么奇技淫巧？</a></p></blockquote><h3 id="一个简单的RISC硬件组成"><a href="#一个简单的RISC硬件组成" class="headerlink" title="一个简单的RISC硬件组成"></a>一个简单的RISC硬件组成</h3><p>麻雀虽小，五脏俱全。根据冯诺依曼理论，一个计算机包括存储器、运算器、控制器以及输入输出设备。该节介绍的极简RISC硬件也具有这些，当然不包括输入输出设备。下面将用Verilog去实现这些硬件电路，完成一个简单的RISC处理器。</p><h4 id="存储器-Memory"><a href="#存储器-Memory" class="headerlink" title="存储器 (Memory)"></a>存储器 (Memory)</h4><ul><li>寄存器列表（RF）<figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> regfile #(</span><br><span class="line">    <span class="keyword">parameter</span> REG_DATA_WIDTH = <span class="number">4</span>,</span><br><span class="line">    <span class="keyword">parameter</span> REG_ADDR_WIDTH = <span class="number">16</span>,</span><br><span class="line">    <span class="keyword">parameter</span> REG_NUMBER     = <span class="number">16</span></span><br><span class="line">)</span><br><span class="line">(</span><br><span class="line">    <span class="keyword">input</span> [REG_ADDR_WIDTH-<span class="number">1</span>:<span class="number">0</span>] rs1_addr,</span><br><span class="line">    <span class="keyword">input</span> [REG_ADDR_WIDTH-<span class="number">1</span>:<span class="number">0</span>] rs2_addr,</span><br><span class="line">    <span class="keyword">input</span> [REG_ADDR_WIDTH-<span class="number">1</span>:<span class="number">0</span>] rd_addr,</span><br><span class="line">    <span class="keyword">input</span> [REG_DATA_WIDTH-<span class="number">1</span>:<span class="number">0</span>] rd_data,</span><br><span class="line">    <span class="keyword">input</span> RegWEn,</span><br><span class="line">    <span class="keyword">input</span> clk,rst_n,</span><br><span class="line"></span><br><span class="line">    <span class="keyword">output</span> [REG_DATA_WIDTH-<span class="number">1</span>:<span class="number">0</span>] rs1_data,</span><br><span class="line">    <span class="keyword">output</span> [REG_DATA_WIDTH-<span class="number">1</span>:<span class="number">0</span>] rs2_data</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">reg</span> [REG_DATA_WIDTH-<span class="number">1</span>:<span class="number">0</span>] rf [REG_NUMBER-<span class="number">1</span>:<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reg0 is always equal to zero</span></span><br><span class="line">    <span class="keyword">assign</span> rs1_data = rs1_addr == <span class="number">0</span>? <span class="number">0</span>:rf[rs1_addr];</span><br><span class="line">    <span class="keyword">assign</span> rs2_data = rs2_addr == <span class="number">0</span>? <span class="number">0</span>:rf[rs2_addr];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">always</span>@ (<span class="keyword">posedge</span> clk <span class="keyword">or</span> <span class="keyword">negedge</span> rst_n) <span class="keyword">begin</span></span><br><span class="line">        <span class="keyword">if</span> (!rst_n) <span class="keyword">begin</span>: REGFILE</span><br><span class="line">            <span class="keyword">integer</span> i;</span><br><span class="line">            <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;REG_NUMBER; i=i+<span class="number">1</span>) <span class="keyword">begin</span></span><br><span class="line">                rf[i] &lt;= <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (RegWEn &amp;&amp; rd_addr!=<span class="number">0</span>) <span class="keyword">begin</span></span><br><span class="line">            rf[rd_addr] &lt;= rd_data;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure></li><li>数据存储器（DCM）<figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> dcmen #(</span><br><span class="line">    <span class="keyword">parameter</span> MEM_ADDR_WIDTH = <span class="number">5</span>,</span><br><span class="line">    <span class="keyword">parameter</span> MEM_DATA_WIDTH = <span class="number">16</span>,</span><br><span class="line">    <span class="keyword">parameter</span> MEM_NUMBER     = <span class="number">32</span></span><br><span class="line">)</span><br><span class="line">(</span><br><span class="line">    <span class="keyword">input</span> clk,</span><br><span class="line">    <span class="keyword">input</span> MemWEn,</span><br><span class="line">    <span class="keyword">input</span> [MEM_ADDR_WIDTH-<span class="number">1</span>:<span class="number">0</span>] addr,</span><br><span class="line">    <span class="keyword">input</span> [MEM_DATA_WIDTH-<span class="number">1</span>:<span class="number">0</span>] dataw,</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">output</span> [MEM_DATA_WIDTH-<span class="number">1</span>:<span class="number">0</span>] datar</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">reg</span> [MEM_DATA_WIDTH-<span class="number">1</span>:<span class="number">0</span>]RAM [MEM_NUMBER-<span class="number">1</span>:<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">always</span> @(<span class="keyword">posedge</span> clk) <span class="keyword">begin</span></span><br><span class="line"><span class="keyword">if</span> (MemWEn) <span class="keyword">begin</span></span><br><span class="line">RAM[addr] &lt;= dataw;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assign</span> datar = RAM[addr] ;</span><br><span class="line"></span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure></li><li>指令存储器（ICM）<figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> icmem #(</span><br><span class="line"><span class="keyword">parameter</span> PC_WIDTH       = <span class="number">16</span>,</span><br><span class="line"><span class="keyword">parameter</span> ISA_WIDTH      = <span class="number">16</span>,</span><br><span class="line"><span class="keyword">parameter</span> MEM_ADDR_WIDTH = <span class="number">5</span>,</span><br><span class="line">    <span class="keyword">parameter</span> MEM_DATA_WIDTH = <span class="number">16</span>,</span><br><span class="line">    <span class="keyword">parameter</span> MEM_NUMBER     = <span class="number">32</span></span><br><span class="line">)</span><br><span class="line">(</span><br><span class="line"><span class="keyword">input</span> clk,</span><br><span class="line"><span class="keyword">input</span> rst_n,</span><br><span class="line"><span class="keyword">input</span> inst_wen,</span><br><span class="line"><span class="keyword">input</span> [ISA_WIDTH-<span class="number">1</span>:<span class="number">0</span>] input_inst,</span><br><span class="line"></span><br><span class="line"><span class="keyword">output</span> [ISA_WIDTH-<span class="number">1</span>:<span class="number">0</span>] current_inst</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">reg</span> [PC_WIDTH-<span class="number">1</span>:<span class="number">0</span>] pc;</span><br><span class="line"></span><br><span class="line"><span class="keyword">always</span> @(<span class="keyword">posedge</span> clk <span class="keyword">or</span> <span class="keyword">negedge</span> rst_n) <span class="keyword">begin</span></span><br><span class="line"><span class="keyword">if</span> (!rst_n) <span class="keyword">begin</span></span><br><span class="line"><span class="comment">// reset</span></span><br><span class="line">pc &lt;= <span class="number">0</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">begin</span></span><br><span class="line">pc &lt;= pc + <span class="number">1</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">dcmen #(</span><br><span class="line"><span class="variable">.MEM_ADDR_WIDTH</span>(MEM_ADDR_WIDTH),</span><br><span class="line"><span class="variable">.MEM_DATA_WIDTH</span>(MEM_DATA_WIDTH),</span><br><span class="line"><span class="variable">.MEM_NUMBER</span>(MEM_NUMBER)</span><br><span class="line">) inst_dcmen (</span><br><span class="line"><span class="variable">.clk</span>    (clk),</span><br><span class="line"><span class="variable">.MemWEn</span> (inst_wen),</span><br><span class="line"><span class="variable">.addr</span>   (pc),</span><br><span class="line"><span class="variable">.dataw</span>  (input_inst),</span><br><span class="line"><span class="variable">.datar</span>  (current_inst)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="运算器-ALU"><a href="#运算器-ALU" class="headerlink" title="运算器 (ALU)"></a>运算器 (ALU)</h4><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> MAC_ALU #(</span><br><span class="line"><span class="keyword">parameter</span> REG_DATA_WIDTH = <span class="number">16</span></span><br><span class="line">)</span><br><span class="line">(</span><br><span class="line"><span class="keyword">input</span> clk,</span><br><span class="line"><span class="keyword">input</span> funct,</span><br><span class="line"><span class="keyword">input</span> [REG_DATA_WIDTH-<span class="number">1</span>:<span class="number">0</span>] rs1,rs2,</span><br><span class="line"></span><br><span class="line"><span class="keyword">output</span> [REG_DATA_WIDTH-<span class="number">1</span>:<span class="number">0</span>] rd</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">wire</span> [REG_DATA_WIDTH-<span class="number">1</span>:<span class="number">0</span>] product;</span><br><span class="line"><span class="keyword">wire</span> [REG_DATA_WIDTH-<span class="number">1</span>:<span class="number">0</span>] addend1,addend2;</span><br><span class="line"><span class="keyword">reg</span>  [REG_DATA_WIDTH-<span class="number">1</span>:<span class="number">0</span>] psum;</span><br><span class="line"></span><br><span class="line"><span class="keyword">assign</span> product = <span class="built_in">$signed</span>(rs1) * <span class="built_in">$signed</span>(rs2);</span><br><span class="line"><span class="keyword">assign</span> addend1 = funct? <span class="number">0</span>: product;</span><br><span class="line"><span class="keyword">assign</span> addend2 = funct? rs2: psum;</span><br><span class="line"><span class="keyword">assign</span> rd = addend1 + addend2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">always</span> @(<span class="keyword">posedge</span> clk) <span class="keyword">begin</span></span><br><span class="line"><span class="keyword">if</span> (funct) <span class="keyword">begin</span></span><br><span class="line">psum &lt;= <span class="number">0</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">begin</span></span><br><span class="line">psum &lt;= rd;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure><p>关于Verilog语法中*的使用，可以参考以下链接</p><blockquote><p><a href="https://www.zhihu.com/question/309627605/answer/580585117">在Verilog中直接调用*实现乘法器，其延迟和占用资源如何？</a></p></blockquote><h4 id="控制器-IDU"><a href="#控制器-IDU" class="headerlink" title="控制器 (IDU)"></a>控制器 (IDU)</h4><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h4 id="极简指令集与其硬件组成"><a href="#极简指令集与其硬件组成" class="headerlink" title="极简指令集与其硬件组成"></a>极简指令集与其硬件组成</h4><table><thead><tr><th align="center">opcode</th><th align="center">目标Reg</th><th align="center">源寄存器/立即数</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center"><font color="#0099ff" size="4">Load</font></td><td align="center">rd</td><td align="center">rs+imm(5b)</td><td align="center">在rst imm地址 -&gt; rd</td></tr><tr><td align="center"><font color="#A52A2A" size="4">Store</font></td><td align="center">/</td><td align="center">rs(addr)/rs(data)</td><td align="center">rs(data) -&gt; Mem index=rs(地址)</td></tr><tr><td align="center"><font color="#9932CC" size="4">MOV</font></td><td align="center">rd</td><td align="center">imm(9b)</td><td align="center">赋值 -&gt; rd</td></tr><tr><td align="center"><font color="#90EE90" size="4">MAC</font></td><td align="center">rd</td><td align="center">rs1/funct=1</td><td align="center">乘加</td></tr><tr><td align="center"><font color="#90EE90" size="4">MAC</font></td><td align="center">rd</td><td align="center">rs1/funct=1</td><td align="center">乘加</td></tr></tbody></table><p>注明：不同颜色的数据通路对应其相同颜色的指令</p><p><img src="/2020/08/21/week3/hw_organization.png" alt></p><h3 id="HomeWork"><a href="#HomeWork" class="headerlink" title="HomeWork"></a>HomeWork</h3>]]></content>
      
      
      <categories>
          
          <category> Course </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 课程笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dark Silicon and ISA</title>
      <link href="/2020/07/14/week2/"/>
      <url>/2020/07/14/week2/</url>
      
        <content type="html"><![CDATA[<p><strong>Week2</strong>：本周学习的知识主要包括：首先介绍了芯片工艺制造过程中缩放定律（Dennard Scaling）的变迁，在该变迁过程中，由于受到功耗的限制，产生了所谓的暗硅现象（Dark Silicon）；其次介绍了指令集架构（ISA）的概念和研究内容，详细阐述了CISC和RISC的区别，并介绍了一个评判ISA性能的理论公式；最后介绍了一个4条指令的极简RISC-V指令集的案例。<a id="more"></a></p><h2 id="内容回顾"><a href="#内容回顾" class="headerlink" title="内容回顾"></a>内容回顾</h2><h3 id="缩放定律的变迁"><a href="#缩放定律的变迁" class="headerlink" title="缩放定律的变迁"></a>缩放定律的变迁</h3><ul><li><p><strong>摩尔定律（1965）的解释：Dennard Scaling</strong></p><p>由美国科学家罗伯特唐纳德（DRAM的发明人）在1974年提出。指出随着芯片尺寸的进一步减小，内部的电场是保持不变的，即<strong>电场守恒缩放（Constant field Scaling）</strong>。因此随着芯片尺寸的虽小，假设尺寸变化比例为$S$。</p><table><thead><tr><th>Transistors Property</th><th align="center">Change</th></tr></thead><tbody><tr><td>${\Delta}$Quantity</td><td align="center">$S^2$</td></tr><tr><td>${\Delta}$Frequency</td><td align="center">$S$</td></tr><tr><td>${\Delta}$Capacity</td><td align="center">$\frac{1}{S}$</td></tr><tr><td>${\Delta}{V_{dd}}^2$</td><td align="center">$\frac{1}{S^2}$</td></tr></tbody></table><p>那么根据功耗的表达式，芯片功耗的变化为<br>$${\Delta}Power={\Delta}QFCV^2=1$$因此，在功耗可以保持不变的情况下，芯片的尺寸一直印证着摩尔定律在不断地缩小，性能也在不断地提高。</p></li><li><p><strong>缩放定律的变迁：Post Dennard Scaling</strong></p><ul><li>原因：晶体管是有阈值电压的，电源电压是有一个极限的，它不能小于管子的阈值电压$V_t$，故电源电压不可能一直减小，最后会保持趋于一个定值。</li><li>发展：故传统Dennard Scaling不再适用，因此，<strong>电场守恒缩放（Constant field Scaling）</strong>逐步转变为<strong>电压守恒缩放（Coinstant Voltage Scaling）</strong>，那么原先的功耗结果将发生变化，即<br>$${\Delta}Power={\Delta}QFCV^2=S^2$$</li></ul><p>这就会造成随着芯片尺寸的减小，芯片的功耗将呈$S^2$倍的变化，导致芯片的功耗急剧增加。如图所示，当尺寸在0.1um的节点中，芯片单位面积产生的功耗已经可以和核电站的发热量相当，这使得了芯片的发展受到了自身功耗的很大限制，也产生了所谓的<strong>暗硅现象</strong>。<br><img src="/2020/07/14/week2/0002.png" alt=" "></p></li></ul><h3 id="暗硅现象"><a href="#暗硅现象" class="headerlink" title="暗硅现象"></a>暗硅现象</h3><ul><li><p><strong>暗硅现象的出现</strong>：由于芯片受到功耗的限制，对于多核处理器而言，他的每一个核并不能全部使用。因此在处理器工作时，有些处理器核是关闭，即所谓的“暗硅”（Dark Silicon）。但是芯片的工艺节点仍然在发展，如今已经到了7nm时代，这意味着我在相同面积的基础上能造更多的处理器核，但是实际工作的处理器核的利用率却越来越低。因此，为了避免以上现象的产生，工业界采用的解决方案是让处理器核处于“灰度状态”（Dim），即让处理器核在更低频率、更低功耗的状态下去工作，这样在消耗相同的功耗情况下，能同时运行的处理器核数可以增加，使得处理器核的利用率提高。这也是目前的处理器主频并没有提高太多的原因。<br><img src="/2020/07/14/week2/0003.png" alt=" "></p></li><li><p><strong>暗硅现象的发展</strong>：基于以上分析，由于Dark Silicon的存在，可以看出一个处理器芯片没有必要造更多的核数量，因为多余的核也是白白浪费的。然而，事实上却截然相反，Dark Silicon的存在反而使得在芯片上出现了更多的处理器核数量，这其中的原因主要如下：</p><ul><li>商业行为：误导消费者处理器核数量越多，处理器性能越强；</li><li>更好地散热：芯片越大，有利于更好地散热；</li><li>制造成本：芯片设计成本较高，制造成本较低。处理器核数量越多，开核的自由度越高，有利于减少处理器的设计难度（增加处理器核之间的距离，帮助更好地散热，更好地优化能耗。例如处理器核通过类似于国际象棋的排布，其热量就不会过于集中，且特定的情况下，在部分极短的时间开启所有核满足性能需求，也不会造成热的积累）。</li></ul><p><img src="/2020/07/14/week2/0004.png" alt=" "></p></li><li><p><strong>专用处理器的设计</strong>：对于目前的处理器而言，芯片功耗的降低比面积的降低更加重要。而使用专用的处理器核相比通用的处理器核能效比更高，产生的功耗也更加低，因此为了更好地提高处理器的性能，在多余核无用的情况下，不如将无用核的替换成专用的处理核，如现在SoC处理器中，一颗多核的CPU周边会搭配若干个专用的处理器核，如GPU、AI加速器、通信基带、视频编解码器等，这样当设备需要什么功能时直接调用相应的处理器核即可，这样将大大减少芯片的功耗，同时提高处理器的性能。因此，从一定程度上可以说，Dark Silicon的出现也为设计专业的处理器核奠定了一定的理论依据。<br><img src="/2020/07/14/week2/0005.png" alt=" "></p></li></ul><h3 id="指令集架构"><a href="#指令集架构" class="headerlink" title="指令集架构"></a>指令集架构</h3><ul><li><p>计算机体系结构研究的内容：</p><ul><li>指令集架构（Instruction Set ArchitectureISA）</li><li>硬件组成（Hardware Organization）</li></ul><p><img src="/2020/07/14/week2/0007.png" alt=" "></p></li><li><p>x86的64位指令集是由AMD发明的</p></li><li><p>RISC和CISC的重要区别：<strong>存储器的访问方式</strong></p><ul><li>RISC只能以load 和store指令访问存储器，而ALU能访问到寄存器列表，数据需要先从存储器加载到寄存中，是Load-Store类型；</li><li>CISC的指令都可以访问到存储器，ALU也能直接访问储器，是Register-Memory类型</li></ul><p><img src="/2020/07/14/week2/0010.png" alt=" "></p></li><li><p>评估一个ISA性能的理论公式<br>$$\frac{Time}{Program}={\frac{Instructions}{Program}}\times{\frac{Clock cycles}{Instructio}}\times{\frac{Time}{Clock cycles}}$$<br><img src="/2020/07/14/week2/0011.png" alt=" "></p></li></ul><h3 id="RISC-V指令集"><a href="#RISC-V指令集" class="headerlink" title="RISC-V指令集"></a>RISC-V指令集</h3><ul><li>RISC-V指令集<br><img src="/2020/07/14/week2/0013.png" alt=" "></li><li>指令集格式说明<br><img src="/2020/07/14/week2/0014.png" alt=" "></li><li>一个4条指令的案例<br><img src="/2020/07/14/week2/0015.png" alt=" "><br><img src="/2020/07/14/week2/0016.png" alt=" "></li></ul>]]></content>
      
      
      <categories>
          
          <category> Course </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 课程笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI core &amp; RISC Architecture 课程笔记</title>
      <link href="/2020/07/05/week1/"/>
      <url>/2020/07/05/week1/</url>
      
        <content type="html"><![CDATA[<p><strong>Week1</strong>：本周主要介绍了人工智能时代下计算机体系结构的现状、计算机的发展历史以及早期神经科学的发展与神经网络（多层感知机）的原理。<a id="more"></a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本系列主要是对 AI core and RISC Architecture 这门课程的学习总结，该课程是由复旦大学陈迟晓老师讲授，课程内容涉及人工智能时代下计算机体系结构的现状与发展，内容丰富，十分前沿，很值得学习。</p><blockquote><p>课程链接 &gt;&gt;&gt; <a href="https://elearning.fudan.edu.cn/courses/26051">AI core and RISC Architecture</a></p></blockquote><h2 id="内容回顾"><a href="#内容回顾" class="headerlink" title="内容回顾"></a>内容回顾</h2><h3 id="课程简介"><a href="#课程简介" class="headerlink" title="课程简介"></a>课程简介</h3><ul><li><p><strong>课程目的</strong><br>传统的通用计算平台已无法满足目前快速发展的AI算法的计算性能，因此需要一种新的硬件计算平台，本课程正是讲解人工智能时代背景下计算机体系结构的现状与发展，通过分析人工智能算法的计算原理，学习目前专用于人工智能计算的硬件体系结构与设计方法。<br><img src="/2020/07/05/week1/0003.png" alt=" hahahaha！ "></p></li><li><p><strong>课程内容</strong></p><ul><li>AI-Core Architecture<br>• Parallelism : Pipeline, Superscalar, SIMD<br>• Architecture: ILP, GPU, CGRA, Neuromorphic<br>• Optimization: data flow, quantization, sparsity<br>• Computing-in-Memory Architecture</li><li>RISC-V Architecture<br>• Instruction Set Architecture<br>• Single-Cycle RV32I Architecture<br>• Pipelined RV32I and hazards</li></ul></li><li><p><strong>参考书籍</strong><br><img src="/2020/07/05/week1/0005.png" alt=" "></p></li></ul><h3 id="计算机的发展历史"><a href="#计算机的发展历史" class="headerlink" title="计算机的发展历史"></a>计算机的发展历史</h3><ul><li><strong>Early Ages – Great Ideas</strong></li><li><strong>The Birth of Computers</strong></li><li><strong>Von-Neumann Architecture</strong></li><li><strong>First CPU Chip（Intel 4004）</strong><br>  <img src="/2020/07/05/week1/0007.png" alt=" "><br>  <img src="/2020/07/05/week1/0008.png" alt=" "><br>  <img src="/2020/07/05/week1/0009.png" alt=" "><br>  <img src="/2020/07/05/week1/0010.png" alt=" "></li></ul><h3 id="神经科学的早期发展与神经网络"><a href="#神经科学的早期发展与神经网络" class="headerlink" title="神经科学的早期发展与神经网络"></a>神经科学的早期发展与神经网络</h3><ul><li><strong>Discovery of Neurons</strong></li><li><strong>Neuron Structure and Modeling</strong></li><li><strong>Visual Cortex in Cats</strong></li><li><strong>Artificial Neural Networks</strong><br>  <img src="/2020/07/05/week1/0012.png" alt=" "><br>  <img src="/2020/07/05/week1/0013.png" alt=" "><br>  <img src="/2020/07/05/week1/0014.png" alt=" "><br>  <img src="/2020/07/05/week1/0015.png" alt=" "></li></ul>]]></content>
      
      
      <categories>
          
          <category> Course </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 课程笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Project Review-4 本科毕业设计</title>
      <link href="/2020/07/04/Review-final/"/>
      <url>/2020/07/04/Review-final/</url>
      
        <content type="html"><![CDATA[<p>该项目是我的本科毕业设计，主要设计了一种卷积神经网络（用于手写数字识别）推理的硬件加速器，使用 16 位定点数对网络参数进行量化，实验基于ZYNQ（XC7Z010）硬件平台，完成了加速器的实现与测试，最终使用 MNIST 测试集的准确率为 98.42%,与Tensorflow 32 位浮点数计算结果仅相差 0.05%；与CPU(i5-6200U)推理速度相比，单张图片推理加速比达到了84.8倍，10000 张图片推理加速比达到了43.6倍。</p><a id="more"></a> <h2 id="项目回顾"><a href="#项目回顾" class="headerlink" title="项目回顾"></a>项目回顾</h2><h3 id="项目大纲"><a href="#项目大纲" class="headerlink" title="项目大纲"></a>项目大纲</h3><p>首先是整个项目的总体回顾，整体工作的内容大纲如下：</p><iframe src="Outline.html" width="600" height="240"></iframe><h3 id="实验平台搭建"><a href="#实验平台搭建" class="headerlink" title="实验平台搭建"></a>实验平台搭建</h3><p>最终的实验平台是基于以下设计方案搭建，主要包括主要围绕 PC 与 ZYNQ、ARM 与 FPGA 这两组设备的通信展开，信号传输包括数据流和控制流。其中，PC 与 ZYNQ 之间的数据采用基于 UDP 协议的网口进行传输，而 ARM 与 ZYNQ 之间的数据采用片内的 AXI 总线和接口传输。<br><img src="/2020/07/04/Review-final/test_arch.png" alt="图1：测试平台的架构设计"><br><img src="/2020/07/04/Review-final/SoC.png" alt="图2：加速器SoC框图"><br><img src="/2020/07/04/Review-final/real_platform.png" alt="图3：实际测试平台的搭建"></p><h3 id="加速器的实验测试"><a href="#加速器的实验测试" class="headerlink" title="加速器的实验测试"></a>加速器的实验测试</h3><h4 id="功能测试"><a href="#功能测试" class="headerlink" title="功能测试"></a>功能测试</h4><p>加速器的功能测试是基于图3搭建的软硬件协同工作平台，实验中使用 10000 张测试集图片进行测试，最终识别正确的图片为 9842 张，即测试集的识别准确率为 98.42%，而 Tensorflow 32 位浮点数计算的识别准确率为 98.47%，两者仅相差 0.05%。其 PC 终端显示的结果如图4所示：<br><img src="/2020/07/04/Review-final/result.png" alt="图4：MNIST准确率测试结果"></p><table><thead><tr><th align="center">Tensorflow准确率</th><th align="center">加速器准确率</th><th align="center">准确率误差</th></tr></thead><tbody><tr><td align="center">98.47%</td><td align="center">98.42%</td><td align="center">0.05%</td></tr></tbody></table><h4 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h4><p>实验测试内容包括单张图片计算时间测试和10000张图片计算时间测试，并与笔记本CPU(i5-6200U)计算时间进行了对比。最终计算性能对比如下：</p><table><thead><tr><th align="center">测试案例</th><th align="center">CPU</th><th align="center">CNN加速器</th><th align="center">加速比</th></tr></thead><tbody><tr><td align="center">单张图片/${\mu}s$</td><td align="center">3985</td><td align="center">47</td><td align="center">84.8</td></tr><tr><td align="center">10000张图片/$ms$</td><td align="center">21455</td><td align="center">497</td><td align="center">43.2</td></tr></tbody></table><p>硬件资源消耗方面，该加速器的资源消耗情况如下表所示。其中 DSP 和LUT 资源消耗最多，DSP 资源利用率达到了 100%，这是因为在卷积计算单元中需要消耗较多的硬件乘法器，而加法树中的加法器硬件都是由LUT实现的。</p><table><thead><tr><th align="center">Resource</th><th align="center">Utilization</th><th align="center">Available</th><th align="center">Utilization / %</th></tr></thead><tbody><tr><td align="center">LUT</td><td align="center">13619</td><td align="center">17600</td><td align="center">77.38</td></tr><tr><td align="center">LUTRAM</td><td align="center">678</td><td align="center">6000</td><td align="center">11.3</td></tr><tr><td align="center">FF</td><td align="center">12594</td><td align="center">35200</td><td align="center">35.78</td></tr><tr><td align="center">BRAM</td><td align="center">29</td><td align="center">60</td><td align="center">48.33</td></tr><tr><td align="center">DSP</td><td align="center">80</td><td align="center">80</td><td align="center">100</td></tr></tbody></table><p>功耗方面，利用 Vivado 功耗估计器（XPE） 进行功耗的评估，最终得到加速器的总功耗为 1.991W。各部分的功耗分布如图5所示。其中，动态功耗包括时钟、信号、逻辑、片上存储、DSP 和 PS 端等资源消耗的功率，共计 1.863W，占整个加速器功耗的94%；静态功耗表示正常工作时晶体管漏电流消耗的功率，主要由芯片的设计本身决定，共计 0.129W，占整个加速器功耗的6%。加速器的功耗为总功耗减去PS侧的功耗，故实际加速器的功耗为0.458W。</p><div align="center">    <img src="/2020/07/04/Review-final/power.svg" width="420" height="300"><center style="font-size:14px"> 图5：加速器功耗分布情况 </center><br></div>]]></content>
      
      
      <categories>
          
          <category> Project </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 毕设项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Project Review-3 本科毕业设计</title>
      <link href="/2020/04/23/Quantization/"/>
      <url>/2020/04/23/Quantization/</url>
      
        <content type="html"><![CDATA[<p>在神经网络中，存在着大量的参数，如果这些参数都用浮点数表示，将会消耗大量的硬件资源，这对于硬件资源有限的嵌入式平台是十分不友好的。在实际硬件实现中，往往采用将浮点数进行定点化处理，用定点数来表示浮点数。</p><a id="more"></a><h2 id="数据的量化与截位"><a href="#数据的量化与截位" class="headerlink" title="数据的量化与截位"></a>数据的量化与截位</h2><h3 id="数据的量化"><a href="#数据的量化" class="headerlink" title="数据的量化"></a>数据的量化</h3><h4 id="浮点数的定点化表示"><a href="#浮点数的定点化表示" class="headerlink" title="浮点数的定点化表示"></a>浮点数的定点化表示</h4><p>在目前的神经网络量化策略中，一般采用16位的定点数来表示数据，且对网络的精度损失比较低。因此，本项目中均采用16位的定点小数来表示输入值和参数。16位的定点数一共有3部分组成，1位符号位，4位整数位，11位小数位。小数位的位数也称量化系数（若小数位宽为0，则该定点数只有整数部分，此时也称为定点整数）。为了便于表示，本文将有符号定点数的表示方式定义如下：<br>$$fix = mQn$$ </p><p>其中，$m$ 表示 定点数的位宽，$n$ 表示小数位宽，即量化系数。</p><p>那么，其浮点数的定点化公式转化如公式1所示：<br>$$ fix = round(float\times2^{n})\tag{1}$$</p><p>其python代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 浮点数量化为定点数(dec)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fp2fix</span>(<span class="params">float_num, quant_width=<span class="number">11</span></span>):</span></span><br><span class="line">    fix_num = <span class="built_in">round</span>(float_num * (<span class="number">2</span>**quant_width))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fix_num</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定点数(dec)转化为浮点数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fix2fp</span>(<span class="params">fix_num, bit_width=<span class="number">16</span>, quant_width=<span class="number">11</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> (fix_num &gt;= <span class="number">2</span>**(bit_width<span class="number">-1</span>)):</span><br><span class="line">        float_num = (fix_num - <span class="number">2</span>**bit_width) / <span class="number">2</span>**quant_width</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        float_num = fix_num / <span class="number">2</span>**quant_width</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> float_num</span><br></pre></td></tr></table></figure><h4 id="定点数的位数扩展"><a href="#定点数的位数扩展" class="headerlink" title="定点数的位数扩展"></a>定点数的位数扩展</h4><p>在定点数的计算中，由于定点数的表示范围有限，对于$mQn$ 定点数的表示范围为<br>$$<br>-2^{m-n-1} \sim 2^{m-n-1}- \frac {1}{2^n}$$ </p><p>例如16$Q$11的数据表示范围为 $-16 \sim 15.9995$。</p><p>在进行加法和乘法操作时，原先的位数已经无法正确表示计算后的结果，因此需要对数据进行位数的扩展。对于有符号定点数而言，在进行扩位时，整数部分需要进行符号位的扩展，小数部分则需要在末尾进行0补充。例如，将 $4Q2$ 定点数扩位成 $6Q3$ 的定点数，其具体扩位操作如下：<br>$$<br>\begin{align}<br>&amp;(4Q2)4’b 10.11=1\times(-2^1)+0\times2^0+1\times2^{-1}+1*2^{-2}=-1.25\\<br> &amp;(6Q3)6’b 110.110 =1\times(-2^2)+1\times2^1+0\times2^0+1\times2^{-1}+1\times2^{-2}+0\times2^{-3}=<br>-1.25\\<br>\end{align}$$</p><p>由上式可知，数据经过扩位后数据大小并不发生变化。</p><h3 id="数据的截位"><a href="#数据的截位" class="headerlink" title="数据的截位"></a>数据的截位</h3><p>在定点数的加法和乘法运算中，为了保证数据的准确性，计算结果都是要进行扩位来保存的，但在实际中由于硬件资源有限，随着数据的频繁计算，不可能一直通过数据扩位来保存数据，因此在保证数据正确性的情况下，需要对数据进行截位处理。一种比较精确的处理方式是先对截位后的数据进行四舍五入处理，如果四舍五入过程中由于进位导致数据溢出，还需要对结果做饱和处理。</p><h4 id="定点数的运算"><a href="#定点数的运算" class="headerlink" title="定点数的运算"></a>定点数的运算</h4><p>两个有符号定点数相加，两个加数一定要对齐小数点并进行符号位的拓展，为了保证数据不溢出，和的总位宽为加数的位宽+1。例如 $aQm$ 和 $bQn$ 相加（$a&gt;b,m&gt;n$），则两者的小数位宽要统一为m位，整数位宽统一为a位，且和的存储格式要用 $(a+1)Qm$存储。</p><p>两个有符号定点数相乘，为了保证积不溢出，积的总位宽为两个乘数位宽之和。例如 $aQm$ 和 $bQn$ 相乘, 则积的存储格式要用 $(a+b)Q(m+n)$ 存储。</p><h4 id="四舍五入"><a href="#四舍五入" class="headerlink" title="四舍五入"></a>四舍五入</h4><p>数据的截位主要依据定点数定义的小数位宽进行截位，但是把数据只进行简单的截取会使得结果不够精确，工程上一般采用通过判断符号位然后根据截掉的小数部分进行四舍五入进位的方式来处理。</p><p>对于正数，首先判断截取部分的最高位是否为1，若最高位为1，则需要进位加1；反之不需要进位。对于负数，进位判断正好相反，如果截取部分最高位为1以及其它位也有为1的情况，由于是负数，所以此时不需要进位。但与正数不同的是，负数不进位时需要加1；反之需要进位时不需要加1。其 <em>verilog</em> 代码如下：</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 小数位低位截位 -&gt; 四舍五入进位判断</span></span><br><span class="line"><span class="keyword">assign</span> carry_bit = calc_out[<span class="number">2</span>*dwidth-<span class="number">1</span>]?(calc_out[qwidth-<span class="number">1</span>] &amp; (|calc_out[qwidth-<span class="number">2</span>:<span class="number">0</span>])):calc_out[qwidth-<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">assign</span> dout_round = &#123;calc_out[<span class="number">2</span>*dwidth-<span class="number">1</span>], calc_out[<span class="number">2</span>*dwidth-<span class="number">1</span>:qwidth]&#125; + carry_bit;</span><br></pre></td></tr></table></figure><h4 id="饱和截位"><a href="#饱和截位" class="headerlink" title="饱和截位"></a>饱和截位</h4><p>所谓的饱和截位是指如果计算结果超出了数据格式能表示的最大值，则用最大值来表示这个溢出的数据。同理，如果计算结果超过了数据的最小值，那么就用最小值来代替这个数据。其 <em>verilog</em> 代码如下：</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 整数位高位截位 -&gt; 溢出进行饱和操作</span></span><br><span class="line"><span class="keyword">assign</span> dout = (dout_round[<span class="number">2</span>*dwidth-qwidth:dwidth-<span class="number">1</span>] == &#123;(dwidth-qwidth+<span class="number">2</span>)&#123;<span class="number">1&#x27;b0</span>&#125;&#125; </span><br><span class="line">            || dout_round[<span class="number">2</span>*dwidth-qwidth:dwidth-<span class="number">1</span>] == &#123;(dwidth-qwidth+<span class="number">2</span>)&#123;<span class="number">1&#x27;b1</span>&#125;&#125;)?</span><br><span class="line">            dout_round[-<span class="number">1</span>:<span class="number">0</span>]:&#123;dout_round[<span class="number">2</span>*dwidth-qwidth],&#123;(dwidth-<span class="number">1</span>)&#123;!dout_round[<span class="number">2</span>*dwidth-qwidtdwidthh]&#125;&#125;&#125;;</span><br></pre></td></tr></table></figure><h4 id="卷积计算电路的计算结果对比"><a href="#卷积计算电路的计算结果对比" class="headerlink" title="卷积计算电路的计算结果对比"></a>卷积计算电路的计算结果对比</h4><p>为了验证计算结果的正确性，将之前转化好的定点数进行输入，经过卷积模块计算后，其最终的结果如表1所示。并将卷积电路计算得到的结果与Matlab软件计算结果进行比较，比较结果如下：</p><table><thead><tr><th align="center">fpga fix</th><th align="center">fpga fix-&gt;float</th><th align="center">result on matlab</th><th align="center">error</th></tr></thead><tbody><tr><td align="center">0xffe0</td><td align="center">-0.01562500000</td><td align="center">-0.01510682666</td><td align="center">-0.0005</td></tr><tr><td align="center">0xff96</td><td align="center">-0.05175781250</td><td align="center">-0.05172769939</td><td align="center">-0.0000</td></tr><tr><td align="center">0x0004</td><td align="center">0.001953125000</td><td align="center">0.002844710976</td><td align="center">-0.0009</td></tr><tr><td align="center">0xff91</td><td align="center">-0.05419921875</td><td align="center">-0.05379437370</td><td align="center">-0.0004</td></tr><tr><td align="center">0x001b</td><td align="center">0.013183593750</td><td align="center">0.014041541711</td><td align="center">-0.0009</td></tr><tr><td align="center">0xffda</td><td align="center">-0.01855468750</td><td align="center">-0.01836803962</td><td align="center">-0.0002</td></tr><tr><td align="center">0xffaf</td><td align="center">-0.03955078125</td><td align="center">-0.03915937634</td><td align="center">-0.0004</td></tr><tr><td align="center">0xffd3</td><td align="center">-0.02197265625</td><td align="center">-0.02108133655</td><td align="center">-0.0009</td></tr><tr><td align="center">0xffee</td><td align="center">-0.00878906250</td><td align="center">-0.00842345750</td><td align="center">-0.0003</td></tr></tbody></table><center style="font-size:14px">表1：卷积计算电路的计算结果对比</center><br><p>由上表可知，该卷积电路计算结果基本正确，误差也在可接受范围之内，如果对数据精度有更高的要求，可以对定点数量化系数进行调整。</p>]]></content>
      
      
      <categories>
          
          <category> Project </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 毕设项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Project Review-2 本科毕业设计</title>
      <link href="/2020/04/22/Review2/"/>
      <url>/2020/04/22/Review2/</url>
      
        <content type="html"><![CDATA[<h2 id="报告内容"><a href="#报告内容" class="headerlink" title="报告内容"></a>报告内容</h2><ul><li>卷积计算单元的上板测试</li><li>池化电路的设计与仿真</li><li>卷积层架构的设计与仿真</li><li>卷积层电路的结果验证</li><li>总结和计划<a id="more"></a></li></ul><h3 id="卷积计算单元的上板测试"><a href="#卷积计算单元的上板测试" class="headerlink" title="卷积计算单元的上板测试"></a>卷积计算单元的上板测试</h3><p>卷积计算电路的测试电路主要包括由ARM、输入输出 <em>FIFO<em>、</em>DMA</em> 电路以及卷积加速电路。具体流程为ARM 端发送开始指令给卷积加速器，加速器从 <em>DDR</em> 中读取数据 ，利用<em>FIFO</em> 输入数据和权重，经卷积计算电路后将数据再写入 <em>FIFO</em> 中，并写回 <em>DDR</em> 中，写回后同时反馈给 ARM ， ARM 端再读取数据，通过串口打印出计算后的数据，测试电路的结果图如下：</p><p><img src="/2020/04/22/Review2/ip_test.png" alt></p><center style="font-size:14px"> 图1：卷积计算单元测试电路效果图 </center><h3 id="池化电路的设计与仿真"><a href="#池化电路的设计与仿真" class="headerlink" title="池化电路的设计与仿真"></a>池化电路的设计与仿真</h3><p>池化电路的设计与之前的卷积电路架构类似，也是利用移位寄存器进行滑窗操作，不同的是池化区域的大小为 <em>2x2</em>，且无权重参数，因此不需要乘法矩阵和加法树。本项目中的卷积神经网络采用的是<strong>最大值池化</strong>，因此只要将原先乘法器的位置换成最大值比较器即可，该池化电路设计原理图如下：</p><p><img src="/2020/04/22/Review2/maxpooling.png" alt></p><center style="font-size:14px"> 图2：池化电路设计图 </center><br><p>由于在池化操作中，一般常采用步长为 <em>2</em> 的滑窗池化，所以相比于之前卷积计算中的步长为<em>1</em>，需要在原来的数据剔除电路中进行改进，将其进行一般化，即考虑步长，卷积核大小，输入特征图尺寸因素，得到一个通用的数据剔除电路。具体分析如下: </p><p>为了尽可能减少数据剔除中 <em>FIFO</em> 缓存的等待时间，假设输入图像大小为 ${n}\times{n}$, 卷积核的大小为 ${m}\times{m}$, 步长为 $u$, 读入 $k$ 个数据后自动读出，则正确的输出数据个数为<br>$$<br>l = (\frac{n-m}{u}+1)^2\tag{1}<br>$$<br>又 <em>FIFO</em> 读入速度为<br>$$<br>v1 = \frac{\frac{n-m}{u}+1}{un}\tag{2}<br>$$<br><em>FIFO</em> 读出速度为<br>$$<br>v2 = 1\tag{3}<br>$$<br>故为了保证数据不丢失，则<br>$$<br>\frac{k}{v2-v1} = \frac{l}{v2}\tag{4}<br>$$<br>因此<br>$$<br>k = (1-\frac{n-m+u}{u^2n})({\frac{n-m}{u}+1})^2(向上取整)\tag{5}<br>$$<br>其中 $k$ 值在 <em>FIFO ip</em> 核的 <em>prog full</em> 阈值中设定。根据 <em>prog full</em> 信号，读入 $k$ 个数据后自动读出。这样既保证数据的不丢失，又可以减少缓存带来的等待时间。</p><div style="page-break-after: always;"></div><h3 id="卷积层电路的设计"><a href="#卷积层电路的设计" class="headerlink" title="卷积层电路的设计"></a>卷积层电路的设计</h3><p>本项目中的网络模型结构有5层（除去输入输出层），包括卷积层1，池化层1，卷积层2，池化层2，全连接层。加速卷积层计算是加速该网络的核心，在卷积运算中，我们经常要对数据进行读写操作，因此需要对数据进行复用，根据卷积运算的并行方式，即有如下三种数据复用的方式：</p><p><img src="/2020/04/22/Review2/data_reuse.png" alt></p><center style="font-size:14px"> 图3：卷积运算中的数据复用方式 </center><br><p>考虑到使用的FPGA开发版资源有限（DSP Slice数量为80），我设计了一个并行度为8的卷积层电路，即可同时进行8个卷积计算，我将其称之为卷积核列表（ConvPE List）。卷积核列表中包括8个卷积计算单元，每个计算单元需要9个乘法器，一共使用了72个DSP Slice，仅卷积层DSP利用率达90%。</p><p>同时，为了提高实现并行计算的效果，我将数据位宽统一为128（16x8）位。其中16是单个数据的位宽，8是卷积计算单元的个数。根据卷积神经网络的结构，卷积层含有两层，因此需要采用卷积核列表重复利用的方式，即一个电路供两次卷积层计算使用。通过上述分析，我设计了卷积层硬件电路，如图4所示：</p><p><img src="/2020/04/22/Review2/convlayer.png" alt></p><center style="font-size:14px"> 图4：卷积层电路架构 </center><br><p>根据网络结构，第一层为单通道特征图像输入，卷积核个数为8，则一次卷积核列表计算就可以完成卷积层1 的计算，之后再经过8个 <em>Relu</em> 和池化电路，得到8个通道的图像输出。同时，为了尽可能地减少在数据的读取加载中消耗太多时间，我将卷积层1的输出结果存放在了FPGA的片上RAM中（Block  RAM），以方便后续计算层快速读取数据，具体硬件电路架构图如下：</p><p><img src="/2020/04/22/Review2/convlayer1.png" alt></p><center style="font-size:14px"> 图5：卷积层1电路架构 </center><br><p>对于第二层网络，由于此时输入特征图像的通道数为8，卷积核个数为16，因为开发板的DSP资源有限，因此完成卷积层2的计算需要调用16次的卷积核列表，加法树，<em>Relu</em>和池化电路。数据的输入由之前存放在BRAM的图像缓存和权值缓存构成。具体的硬件电路架构图如下：</p><p><img src="/2020/04/22/Review2/convlayer2.png" alt></p><center style="font-size:14px"> 图6：卷积层2电路架构 </center><br><h3 id="卷积层电路的结果验证"><a href="#卷积层电路的结果验证" class="headerlink" title="卷积层电路的结果验证"></a>卷积层电路的结果验证</h3><p>卷积层电路的验证与调试过程中，主要利用 Tensorflow 训练好的参数进行验证，打印输出各层的结果，并转换为16位的定点数进行比较。电路调试通过在仿真波形图中观察各个控制信号、输入输出数据的长度、数据同步性等。其最终仿真结果的波形图如下：</p><p><img src="/2020/04/22/Review2/convlayer_sim.png" alt></p><center style="font-size:14px"> 图7：卷积层计算仿真 </center><br><p>将上述卷积加速器的计算结果与 Tensorflow 输出的结果进行对比，以其中一个输出通道为例（大小为 6x6)，对比结果如下：</p><ul><li>FPGA卷积加速器计算结果：</li></ul><p>$$<br>\left[\begin{matrix}<br>  0.0000 &amp; 0.0000 &amp; 0.1548 &amp; 0.0000 &amp; 1.4199 &amp; 1.6777\\0.2222 &amp; 1.2314 &amp; 0.6880 &amp; 0.5923 &amp; 2.1265 &amp; 1.4238\\1.2700 &amp; 1.7573 &amp; 0.0000 &amp; 2.1201 &amp; 2.2896 &amp; 0.4580\\1.6187 &amp; 1.3481 &amp; 1.9756 &amp; 3.1812 &amp; 2.0439 &amp; 0.4443\\1.2183 &amp; 0.8091 &amp; 0.7070 &amp; 1.6250 &amp; 0.9170 &amp; 0.0137\\0.0024 &amp; 0.0615 &amp; 0.6807 &amp; 1.8628 &amp; 0.3379 &amp; 0.0000<br>\end{matrix}\right]<br>$$</p><ul><li>Tensorflow计算结果：</li></ul><p>$$<br>\left[\begin{matrix}<br>    0.0000 &amp; 0.0000 &amp; 0.1562 &amp; 0.0000 &amp; 1.4209 &amp; 1.6794\\0.2204 &amp; 1.2317 &amp; 0.6900 &amp; 0.5948 &amp; 2.1252 &amp; 1.4247\\1.2713 &amp; 1.7572 &amp; 0.0000 &amp; 2.1179 &amp; 2.2903 &amp; 0.0122\\1.6156 &amp; 1.3483 &amp; 1.9769 &amp; 3.1808 &amp; 2.0455 &amp; 0.0000\\1.2183 &amp; 0.8096 &amp; 0.7073 &amp; 1.6252 &amp; 0.9191 &amp; 0.0143\\0.0035 &amp; 0.0623 &amp; 0.7224 &amp; 1.8634 &amp; 0.3399 &amp; 0.0000<br>\end{matrix}\right]<br>$$</p><div style="page-break-after: always;"></div><ul><li>两者误差结果：</li></ul><p>$$<br>\left[\begin{matrix}<br>    0.0000 &amp;  0.0000 &amp; -0.0015 &amp;  0.0000 &amp; -0.0010 &amp; -0.0017\\ 0.0018 &amp; -0.0003 &amp; -0.0020 &amp; -0.0026 &amp;  0.0013 &amp; -0.0009\\-0.0013 &amp;  0.0001 &amp;  0.0000 &amp;  0.0022 &amp; -0.0008 &amp;  0.4458\\ 0.0031 &amp; -0.0002 &amp; -0.0013 &amp;  0.0003 &amp; -0.0016 &amp;  0.4443\\-0.0000 &amp; -0.0005 &amp; -0.0002 &amp; -0.0002 &amp; -0.0021 &amp; -0.0007\\-0.0010 &amp; -0.0008 &amp; -0.0417 &amp; -0.0006 &amp; -0.0020 &amp;  0.0000<br>\end{matrix}\right]<br>$$</p><p>由上述结果可知，该卷积层电路计算基本正确，考虑到卷积神经网络具有一定的鲁棒性，因此在误差不大的情况下，对最终的识别准确率影响不会很大。</p><h3 id="总结和计划"><a href="#总结和计划" class="headerlink" title="总结和计划"></a>总结和计划</h3><p>本月主要完成了加速器外围测试电路的搭建，数据剔除电路的通用化设计，卷积层电路的设计与仿真验证工作，对之前的电路进行了完善和改进，使卷积加速器更加通用化，以便可以适应各种不同的卷积计算，具有更广泛的应用前景。之后计划根据搭建好的测试平台，完成整个卷积加速器的上板测试，并与电脑 CPU 计算进行对比，若加速效果达到预期，则开始着手毕业论文的撰写，并进一步完善卷积神经网络加速器。</p>]]></content>
      
      
      <categories>
          
          <category> Project </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 毕设项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Project Review-1 本科毕业设计</title>
      <link href="/2020/04/01/Review1/"/>
      <url>/2020/04/01/Review1/</url>
      
        <content type="html"><![CDATA[<h2 id="报告内容"><a href="#报告内容" class="headerlink" title="报告内容"></a>报告内容</h2><ul><li>卷积神经网络模型的搭建和训练</li><li>模型的参数获取及数据的定点化</li><li>基于 <em>ZYNQ</em> 的卷积神经网络加速器架构的设计</li><li>卷积核计算电路的设计、仿真以及计算结果对比</li><li>总结和计划<a id="more"></a></li></ul><h3 id="CNN-模型的搭建"><a href="#CNN-模型的搭建" class="headerlink" title="CNN 模型的搭建"></a><em>CNN</em> 模型的搭建</h3><p>一种用于手写数字识别的卷积神经网络（<em>Convolution Neural Network</em> , 简称<em>CNN</em> ），参考了<em>LeNet-5</em> 模型，我对该模型的卷积核大小以及数量进行了调整，以便更好适配之后设计的硬件加速器，且两者的识别准确率相差不大。该网络模型包括输入层、卷积层、池化层、全连接层以及输出层。模型结构如图1所示：</p><p><img src="/2020/04/01/Review1/cnnmodel.svg" alt></p><center style="font-size:14px"> 图1：卷积神经网络的结构 </center><br><p>其中输入图像像素大小为<em>30x30</em> ,经过两组卷积层和池化层，最后连接两层全连接层。卷积层采用大小为<em>3x3</em> 的卷积核，步长为<em>1</em>；池化层采用的是<em>2x2</em> 区域的最大值池化；激活函数使用 <em>ReLu</em> 。使用 <strong>Tensorflow</strong>进行网络模型搭建，部分代码如下。在 <strong>Colab</strong> 上已经完成了模型的训练，该模型在<em>10000</em> 张验证集图片中准确率为<strong>98.13%</strong> 。部分源代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搭建网络结构</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MNIST_CNN</span>(<span class="params">Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = layers.Conv2D(</span><br><span class="line">            filters=<span class="number">8</span>,</span><br><span class="line">            kernel_size=[<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">            activation=tf.nn.relu,</span><br><span class="line">            padding=<span class="string">&#x27;valid&#x27;</span>,</span><br><span class="line">            name=<span class="string">&#x27;conv2d_1&#x27;</span>,</span><br><span class="line">            input_shape=(<span class="number">30</span>, <span class="number">30</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        self.pool1 = layers.MaxPool2D(pool_size=[<span class="number">2</span>, <span class="number">2</span>], strides=<span class="number">2</span>, name=<span class="string">&#x27;max_pooling2d_1&#x27;</span>)</span><br><span class="line">        self.conv2 = layers.Conv2D(</span><br><span class="line">            filters=<span class="number">16</span>,</span><br><span class="line">            kernel_size=[<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">            padding=<span class="string">&#x27;valid&#x27;</span>,</span><br><span class="line">            name=<span class="string">&#x27;conv2d_2&#x27;</span>,</span><br><span class="line">            activation=tf.nn.relu</span><br><span class="line">        )</span><br><span class="line">        self.pool2 = layers.MaxPool2D(pool_size=[<span class="number">2</span>, <span class="number">2</span>], strides=<span class="number">2</span>, name=<span class="string">&#x27;max_pooling2d_2&#x27;</span>)</span><br><span class="line">        self.flatten = layers.Reshape(target_shape=(<span class="number">6</span> * <span class="number">6</span> * <span class="number">16</span>,))</span><br><span class="line">        self.dense1 = layers.Dense(units=<span class="number">10</span>)</span><br><span class="line">        <span class="comment"># self.dense2 = layers.Dense(units=10)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        x = self.conv1(inputs)            <span class="comment"># [batch_size, 28, 28, 8]</span></span><br><span class="line">        x = self.pool1(x)                 <span class="comment"># [batch_size, 14, 14, 8]</span></span><br><span class="line">        x = self.conv2(x)                 <span class="comment"># [batch_size, 12, 12, 16]</span></span><br><span class="line">        x = self.pool2(x)                 <span class="comment"># [batch_size, 6, 6, 16]</span></span><br><span class="line">        x = self.flatten(x)               <span class="comment"># [batch_size, 6 * 6 * 16]</span></span><br><span class="line">        x = self.dense1(x)                <span class="comment"># [batch_size, 10]</span></span><br><span class="line">        output = tf.nn.softmax(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line">model = MNIST_CNN()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 验证准确率</span></span><br><span class="line">pred = <span class="string">&#x27;Loss: &#123;:.4f&#125;, Accuracy: &#123;:.4f&#125;&#x27;</span></span><br><span class="line">print(pred.<span class="built_in">format</span>(test_loss.result(),test_accuracy.result()))</span><br></pre></td></tr></table></figure><h3 id="模型的参数获取与数据的定点化"><a href="#模型的参数获取与数据的定点化" class="headerlink" title="模型的参数获取与数据的定点化"></a>模型的参数获取与数据的定点化</h3><p>利用 <strong>Tensorflow</strong> 的 <em>checkpoint</em> 函数可以对训练好的参数文件进行保存，由于在云端训练时采用的是32位的浮点数计算，故保存下来的参数是浮点数格式。但在 <em>FPGA</em> 上进行浮点数计算时，会消耗大量的硬件资源，可能无法在板上有限的硬件资源下完成加速器的硬件电路实现，因此需要将 <strong>浮点数做定点化处理</strong>，本次项目中使用的16位的定点数，包括1位符号位，4位整数位以及11位小数位，利用 <em>python</em>编写了数值转换的脚本，其部分结果如下表所示：</p><table><thead><tr><th align="center">原始浮点数</th><th align="center">定点数</th><th align="center">定点数还原浮点数</th><th align="center">误差值</th></tr></thead><tbody><tr><td align="center">0.10980392</td><td align="center">0x00e1</td><td align="center">0.10986328</td><td align="center">-0.0000594</td></tr><tr><td align="center">0.78039216</td><td align="center">0x063e</td><td align="center">0.78027344</td><td align="center">0.0001187</td></tr><tr><td align="center">0.33333333</td><td align="center">0x02ab</td><td align="center">0.33349609</td><td align="center">-0.0001628</td></tr><tr><td align="center">0.98823529</td><td align="center">0x07e8</td><td align="center">0.98828125</td><td align="center">-0.0000460</td></tr><tr><td align="center">0.97647059</td><td align="center">0x07d0</td><td align="center">0.97656250</td><td align="center">-0.0000919</td></tr><tr><td align="center">0.57254902</td><td align="center">0x0495</td><td align="center">0.57275391</td><td align="center">-0.0002049</td></tr><tr><td align="center">0.18823529</td><td align="center">0x0182</td><td align="center">0.18847656</td><td align="center">-0.0002413</td></tr><tr><td align="center">0.11372549</td><td align="center">0x00e9</td><td align="center">0.11376953</td><td align="center">-0.0000440</td></tr><tr><td align="center">0.33333333</td><td align="center">0x02ab</td><td align="center">0.33349609</td><td align="center">-0.0001628</td></tr><tr><td align="center">0.69803922</td><td align="center">0x0596</td><td align="center">0.69824219</td><td align="center">-0.0002030</td></tr><tr><td align="center">0.91372549</td><td align="center">0x074f</td><td align="center">0.91357422</td><td align="center">0.00015130</td></tr></tbody></table><center style="font-size:14px"> 表1：浮点数与定点数的比较 </center><br><p>上述结果显示将浮点数转成定点数后，误差在$10^{-5}\sim10^{-4}$范围内，因此采用将浮点数转化为定点数表示的方法是可行的。定点数的计算对<em>FPGA</em>硬件资源十分友好，且16位的定点数相比32位的浮点数可以大大减少了存储资源的占用和数据的加载速度。</p><h3 id="基于ZYNQ-的卷积神经网络加速器架构的设计"><a href="#基于ZYNQ-的卷积神经网络加速器架构的设计" class="headerlink" title="基于ZYNQ 的卷积神经网络加速器架构的设计"></a>基于<em>ZYNQ</em> 的卷积神经网络加速器架构的设计</h3><p>实现 <em>CNN</em> 加速器的整体实现流程主要通过 <em>PC</em> 端将手写数字图片发送至 <em>ZYNQ</em> 平台(包括 <em>ARM(PL)</em> 和 <em>FPGA(PL)</em> 即 <em>ARM</em> 接受图片数据并存放在 <em>DDR</em> 中，通过 <em>AXI-GP</em> 接口发送指令给 <em>FPGA</em> ，<em>FPGA</em> 接受到开始指令从 <em>DDR</em> 中读取图片数据和权值偏置参数，并完成卷积神经网络的计算，计算完成后反馈回 <em>ARM</em> ，最后再由 <em>ARM</em> 将计算结果发送回 <em>PC</em> 端并显示识别结果。其整体架构图如图2所示：<br><img src="/2020/04/01/Review1/overall.png" alt></p><center style="font-size:14px"> 图2：加速器整体结构 </center><br><p><em>CNN</em> 加速器的架构主要包括输入缓存，加速器ip以及输出缓存。<em>FPGA</em> 的数据读入主要利用 <em>AXI-HP</em> 接口，从 DDR 中来获取图片数据和参数数据，然后加速器计算完成后再将结果写回 DDR 中，其加速器的架构图如图3所示：<br><img src="/2020/04/01/Review1/ip.png" alt></p><center style="font-size:14px"> 图3：加速器ip外围架构 </center><br><h3 id="卷积核计算电路的设计"><a href="#卷积核计算电路的设计" class="headerlink" title="卷积核计算电路的设计"></a>卷积核计算电路的设计</h3><p>首先，整个卷积计算电路单元模块化设计如图4所示。它包括数据缓存模块、卷积计算模块以及数据剔除模块。<br><img src="/2020/04/01/Review1/convPE_module.png" alt></p><center style="font-size:14px">图4：卷积计算单元</center><br><h4 id="数据缓存模块"><a href="#数据缓存模块" class="headerlink" title="数据缓存模块"></a>数据缓存模块</h4><p>由于输入图像是以数据流形式输入，而卷积计算通过滑窗对卷积核内的数据进行乘累加操作，因此我用三个移位寄存器组将输入数据流分成三行，对应于 <em>3x3</em> 的卷积核。同理，权重数据流也是类似操作，如图5所示：</p><p><img src="/2020/04/01/Review1/shiftRam.png" alt></p><center style="font-size:14px">图5：输入数据拆分操作</center><br><h4 id="卷积计算单元"><a href="#卷积计算单元" class="headerlink" title="卷积计算单元"></a>卷积计算单元</h4><p>卷积计算在硬件电路实现时可以有效利用其并行性，以 <em>3x3</em> 的卷积核为例，常见的硬件实现方案都是使用乘法矩阵和加法树进行计算，以下是两种常见的硬件实现方案：</p><p><img src="/2020/04/01/Review1/convPE.png" alt></p><center style="font-size:14px">图6：两种卷积计算电路方案</center><br><p>根据上述两种方案的设计结构，我采用了使用加法器更少的方案二，这样可以有效地节省 <em>FPGA</em> 的硬件资源，并且该方案的思路可以同时应用于池化层的设计。</p><h4 id="数据剔除模块"><a href="#数据剔除模块" class="headerlink" title="数据剔除模块"></a>数据剔除模块</h4><p>另外，在设计过程中发现，在卷积计算过程中，计算完成一行后会从下一行的第一列重新计算，但实际输入数据以数据流的形式输入，依次输入前后数据是连续的，数据流进行中会产生前一行的末尾数据与后一行的开头数据进行卷积的错误结果，因此需要将这些数据进行剔除。根据仿真波形的观察，错误数据的出现是有规律的，因此利用一个FIFO作为缓存，当数据到来的同时产生一个脉冲控制信号，控制FIFO的写入，通过设置剔除信号的占空比来剔除掉错误数据，将正确的数据写入FIFO中，然后在保证数据完整正确的情况下再从FIFO中读出。<br><img src="/2020/04/01/Review1/correct.png" alt></p><center style="font-size:14px">图7：错误数据的形成</center><br><p>为了尽可能减少数据剔除中FIFO缓存的等待时间，假设输入图像大小为 ${n}\times{n}$, 卷积核的大小为 ${3}\times{3}$, 读入 $k$ 个数据后自动读出，则正确的输出数据个数为<br>$$<br>l = (n-2)^2\tag{1}<br>$$<br>又 FIFO 读入速度为<br>$$<br>v1 = \frac{n-2}{n}\tag{2}<br>$$<br>FIFO 读出速度为<br>$$<br>v2 = 1\tag{3}<br>$$<br>故为了保证数据不丢失, 则<br>$$<br>\frac{k}{v2-v1} = \frac{l}{v2}\tag{4}<br>$$<br>因此<br>$$<br>k = \frac{2(n-2)^2}{n}(向上取整)\tag{5}<br>$$<br>其中 $k$ 值在 FIFO ip核的 prog full 阈值中设定。根据 prog full 信号,读入 $k$ 个数据后自动读出,这样既保证数据的不丢失，又可以减少缓存带来的等待时间。</p><h3 id="卷积计算电路的仿真"><a href="#卷积计算电路的仿真" class="headerlink" title="卷积计算电路的仿真"></a>卷积计算电路的仿真</h3><p>卷积计算电路仿真波形图如图8所示，其中，输入数据流为$\left[\begin{matrix}1 &amp; 2 &amp; \cdots &amp; 5\\6 &amp; 7 &amp; \cdots &amp; 10\\\vdots &amp; \vdots &amp; \ddots &amp;\vdots\\21 &amp; 22 &amp; \cdots &amp; 25 \end{matrix}\right]$, 权值矩阵为$\left[\begin{matrix}1 &amp; 2 &amp; 3\\1 &amp; 2 &amp; 3\\1 &amp; 2 &amp; 3\end{matrix}\right]$。</p><p><img src="/2020/04/01/Review1/convPEsim.png" alt></p><center style="font-size:14px">图8：卷积计算电路的仿真图</center><br><h3 id="总结和计划"><a href="#总结和计划" class="headerlink" title="总结和计划"></a>总结和计划</h3><p>3月主要完成了CNN网络模型的训练和基本卷积电路计算单元的设计，并且完成了数据定点化的工作和卷积计算电路的仿真验证工作,原预期计划内容已基本完成。下一阶段打算完成池化层、全连接层等电路的设计和验证工作，完成整个卷积网络加速器的架构设计，并搭建好加速器外围数据写入和读出的硬件电路，进行上板测试。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul><li>[1] Qiu, Jiantao, et al. “Going deeper with embedded fpga platform for convolutional neural network.” Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. 2016. </li><li>[2] Ma, Yufei, et al. “Scalable and modularized RTL compilation of convolutional neural networks onto FPGA.” 2016 26th International Conference on Field Programmable Logic and Applications (FPL). IEEE, 2016.</li><li>[3] Guo K, Zeng S, Yu J, et al. A survey of fpga-based neural network accelerator[J]. arXiv preprint arXiv:1712.08934, 2017.</li><li>[4] Sankaradas, Murugan, et al. “A massively parallel coprocessor for convolutional neural networks.” 2009 20th IEEE International Conference on Application-specific Systems, Architectures and Processors. IEEE, 2009.</li><li>[5] Zhai, Sheping, et al. “Design of Convolutional Neural Network Based on FPGA.” Journal of Physics: Conference Series. Vol. 1168. No. 6. IOP Publishing, 2019.</li><li>[6] 秦华标,曹钦平.基于FPGA的卷积神经网络硬件加速器设计[J].电子与信息学报,2019,41(11):2599-2605.</li><li>[7] 余子健. 基于FPGA的卷积神经网络加速器[D].浙江大学,2016.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Project </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 毕设项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记</title>
      <link href="/2019/10/30/paper1/"/>
      <url>/2019/10/30/paper1/</url>
      
        <content type="html"><![CDATA[<p><em><strong>Going Deeper with Embedded FPGA Platform for Convolutional Neural Network</strong></em> <br><br><font size="1"> FPGA’16, February 21-23, 2016, Monterey, CA, USA </font></p><a id="more"></a><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li><em>CNN</em> 模型中卷积层是以计算为中心，而全连接层是以内存为中心，<strong>即限制卷积层计算速度的关键在于大量的数据计算，而全连接层相比数据计算而言,限制速度的瓶颈在于数据读取的带宽</strong></li><li>采用动态数据量化方法和一种卷积核的设计实现在整个 <em>CNN</em> 模型，可以提高 <strong>带宽</strong> 和 <strong>资源的利用率</strong></li></ul><h2 id="Platform"><a href="#Platform" class="headerlink" title="Platform"></a>Platform</h2><ul><li>Device: Xilinx Zynq ZC706</li><li><em>CNN</em> Model: VGG16-SVD</li></ul><h2 id="Key-Content"><a href="#Key-Content" class="headerlink" title="Key Content"></a>Key Content</h2><ul><li><p>一种动态数据的量化方法</p></li><li><p>卷积核的3种并行</p><ul><li>乘法器之间的并行</li><li>卷积核运算单元之间的并行</li><li>处理单元PE之间而的并行<br><img src="/2019/10/30/paper1/Parallelism.png" alt="Parallelism Architecture"></li></ul></li><li><p>数据搬移策略</p></li></ul><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2>]]></content>
      
      
      <categories>
          
          <category> Paper notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN Accelerator </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python 代码规范</title>
      <link href="/2019/10/30/python_PEP8_Solutions/"/>
      <url>/2019/10/30/python_PEP8_Solutions/</url>
      
        <content type="html"><![CDATA[<p>记录了Python编程过程中代码规范（PEP8）的问题，并且给出了相应的解决方法。</p><a id="more"></a><h5 id="Python-代码规范-PEP-8-问题及解决"><a href="#Python-代码规范-PEP-8-问题及解决" class="headerlink" title="Python 代码规范 PEP 8 问题及解决"></a><em><strong>Python</strong></em> 代码规范 PEP 8 问题及解决</h5><ol><li><strong>PEP 8: module level import not at top of file</strong> </li></ol><ul><li>Solved：import不在文件的最上面，可能引用之前还有代码，把import引用放到文件的最上部就可以消除警告了。</li></ul><ol start="2"><li><strong>PEP 8: expected 2 blank lines，found 0</strong> </li></ol><ul><li>Solved：期望上面有2个空白行，发现0个，添加两个空白行就可以了。</li></ul><ol start="3"><li><strong>function name should be lowercase</strong> </li></ol><ul><li>Solved：函数名改成小写。</li></ul><ol start="4"><li><strong>PEP 8: indentation contains tabs</strong> </li></ol><ul><li>Solved：缩进中有tab空格，推荐用四个空格缩进。</li></ul><ol start="5"><li><strong>Indent expected</strong> </li></ol><ul><li>Solved：意思是没有缩进，解析器报错了，添加缩进就可以了。</li></ul><ol start="6"><li><strong>Unexpected indent</strong> </li></ol><ul><li>Solved：不期望的缩进，重新添加符合规范的缩进或者Alt+Enter快捷键会提示你转化成规范的缩进。</li></ul><ol start="7"><li><strong>PEP 8: missing whitespace around operator</strong> </li></ol><ul><li>Solved：意思是操作符（‘=’，‘&lt;’等）前后丢失了空格，举个例子a=b会报警告，a = b正常。</li></ul><ol start="8"><li><strong>PEP 8: no newline at end of file</strong> </li></ol><ul><li>Solved：文件尾部没有新起一行，光标移到最后回车即可。</li></ul><ol start="9"><li><strong>PEP 8: blank line at end of file</strong> </li></ol><ul><li>Solved：文件最后多了一个空白行，只要有一个即可，删掉一个。</li></ul><ol start="10"><li><strong>Shadows name ‘xxx’ from outer scope</strong> </li></ol><ul><li>Solved：意思是‘xxx’在外部已经定义了，修改一下‘xxx’-&gt; ‘uuu’或者其他符合要求的修改都可。</li></ul><ol start="11"><li><strong>PEP 8: block comment should start with ‘# ’</strong> </li></ol><ul><li>Solved：说的很清楚要以#加一个空格开始</li></ul><ol start="12"><li><strong>PEP 8: inline comment should start with ‘# ’</strong> </li></ol><ul><li>Solved：注释信息单独放一行</li></ul><ol start="13"><li><strong>PEP 8: multiple statements on one line (colon)</strong> </li></ol><ul><li>Solved：多行语句写到一行了，<em><strong>Python3.0</strong></em> 好像不允许写到一行了，例如if x == 2: print(something)这样写就会有警告，必须要分两行。像下面这样<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if x &#x3D;&#x3D; 2:</span><br><span class="line">    print(something)</span><br></pre></td></tr></table></figure></li></ul><ol start="14"><li><strong>PEP 8: W291 trailing whitespace</strong> </li></ol><ul><li>Solved：出现了多余的空格</li></ul>]]></content>
      
      
      <categories>
          
          <category> Problem sloved </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
